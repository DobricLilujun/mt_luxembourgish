{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luxembourgish language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Lujun LI\n",
    "\n",
    "This is a demo project for luxembourgish langaguage translation \n",
    "\n",
    "We mainly use langchain and huggingface opensource models\n",
    "\n",
    "1. Classification of langaguage: especially luxembourgish\n",
    "2. Translate luxembourgish to english\n",
    "3. Translate non-luxembourgish sentence to english\n",
    "4. Sentiment analysis for the pipeline\n",
    "\n",
    "\n",
    "Objectif: low cost & high accuracy\n",
    "\n",
    "**Start from loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "folder_name = \"data/\"\n",
    "file_name = \"Machine Translation - Luxembourguish_list - Machine Translation - Luxembourguish_list.csv\"\n",
    "sheet_name = \"\"\n",
    "data = pd.read_csv(folder_name + file_name)\n",
    "column_name = \"Sentence\"\n",
    "batch_size = 1000\n",
    "\n",
    "language_map = {\n",
    "    \"de\": \"German\",\n",
    "    \"nl\": \"Dutch\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"pt\": \"Portuguese\",\n",
    "    \"ur\": \"Urdu\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"fr\": \"French\",\n",
    "    \"lb\": \"Luxembourgish\",\n",
    "    \"en\": \"English\",\n",
    "}\n",
    "language_map_reverse = {v: k for k, v in language_map.items()}\n",
    "data[\"true_flag\"] = data[\"Language\"].map(language_map_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "texts = data[column_name].astype(str).to_list()\n",
    "\n",
    "model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "    inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    preds = torch.softmax(logits, dim=-1)\n",
    "    all_preds.append(preds)\n",
    "\n",
    "all_preds = torch.cat(all_preds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2lang = model.config.id2label\n",
    "vals, idxs = torch.max(all_preds, dim=1)\n",
    "labels = []\n",
    "pbs = []\n",
    "for idx, val in zip(idxs, vals):\n",
    "    labels.append(id2lang[idx.item()])\n",
    "    pbs.append(val.item())\n",
    "\n",
    "label_series = pd.Series(labels, name=\"label_roberta\")\n",
    "pb_series = pd.Series(pbs, name=\"probability_roberta\")\n",
    "data = pd.concat([data, label_series, pb_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langid.langid import LanguageIdentifier, model\n",
    "\n",
    "labels = []\n",
    "pbs = []\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "identifier.set_languages([\"lb\", \"en\", \"nl\", \"fr\", \"de\"])\n",
    "for text in texts:\n",
    "    conf, prob = identifier.classify(text)\n",
    "    labels.append(conf)\n",
    "    pbs.append(prob)\n",
    "\n",
    "label_series = pd.Series(labels, name=\"label_langid\")\n",
    "pb_series = pd.Series(pbs, name=\"probability_langid\")\n",
    "data = pd.concat([data, label_series, pb_series], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without training and directly use the model to do the translation from German to english and from luxembourgish to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujun/anaconda3/envs/causalLLM/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = \"etamin/Letz-Translate-OPUS-LB-EN\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = (\n",
    "    data[~data[\"label_roberta\"].isin([\"en\", \"fr\"])][column_name].astype(str).to_list()\n",
    ")\n",
    "indices = data[~data[\"label_roberta\"].isin([\"en\", \"fr\"])].index\n",
    "\n",
    "translated_texts = []\n",
    "\n",
    "for i in range(0, len(input_texts), batch_size):\n",
    "    batch_texts = input_texts[i : i + batch_size]\n",
    "    batch_indices = indices[i : i + batch_size]\n",
    "\n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "    batch_translations = [\n",
    "        tokenizer.decode(output, skip_special_tokens=True) for output in outputs\n",
    "    ]\n",
    "    translated_texts.extend(batch_translations)\n",
    "\n",
    "\n",
    "translated_series = pd.Series(translated_texts, index=indices)\n",
    "data[\"translated_sentence\"] = None\n",
    "data.loc[translated_series.index, \"translated_sentence\"] = translated_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non commercial liscence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# # Mistral Llama3\n",
    "# model_name = \"facebook/nllb-200-distilled-1.3B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# def batch_translate(\n",
    "#     texts, source_lang=\"fra_Latn\", target_lang=\"eng_Latn\", batch_size=500\n",
    "# ):\n",
    "#     translations = []\n",
    "#     for i in range(0, len(texts), batch_size):\n",
    "#         batch_texts = texts[i : i + batch_size]\n",
    "#         inputs = tokenizer(\n",
    "#             batch_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "#         )\n",
    "#         translated_tokens = model.generate(\n",
    "#             **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[target_lang]\n",
    "#         )\n",
    "#         translated_texts = tokenizer.batch_decode(\n",
    "#             translated_tokens, skip_special_tokens=True\n",
    "#         )\n",
    "#         translations.extend(translated_texts)\n",
    "#     return translations\n",
    "\n",
    "\n",
    "# french_texts = (\n",
    "#     data[data[\"translated_sentence\"].isna()][column_name].astype(str).to_list()\n",
    "# )\n",
    "# indices = data[data[\"translated_sentence\"].isna()].index\n",
    "# english_translations = batch_translate(french_texts, batch_size=batch_size)\n",
    "# translated_series = pd.Series(english_translations, index=indices)\n",
    "# data.loc[translated_series.index, \"translated_sentence\"] = translated_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name = \"\"\n",
    "data.to_csv(f\"output_{file_name}_{sheet_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643820bc5efd4993bc846b7aa9da8752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "instruction = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\"\"\"\n",
    "question = \"hello, I am lujun\"\n",
    "template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{instruction}\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{question}\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "prompt = prompt_template.format(instruction=instruction, question=question)\n",
    "generated_text = pipeline(prompt, max_length=250)[0][\"generated_text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
