{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:   0%|          | 3/100902 [00:03<32:37:45,  1.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(csv_file, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# 每次请求后暂停 5 秒\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# print(f\"Saved article {article_number} to CSV.\")\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData has been saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticles.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "url_template = \"https://www.rtl.lu/mobiliteit/news/a/{0}.html\"\n",
    "\n",
    "\n",
    "def is_page_not_found(soup):\n",
    "    return \"404\" in soup.text\n",
    "\n",
    "\n",
    "def extract_date(soup):\n",
    "    metainfo = soup.find(\"div\", class_=\"article-heading__metainfo\")\n",
    "    if metainfo:\n",
    "        return metainfo.get_text(strip=True)\n",
    "    return \"Date Information Is Not Found\"\n",
    "\n",
    "\n",
    "def extract_date_details(date_string):\n",
    "    \n",
    "    match = re.search(r\"Update:\\s*(\\d{2})\\.(\\d{2})\\.(\\d{4})\", date_string)\n",
    "    if match:\n",
    "        day, month, year = map(int, match.groups())\n",
    "        try:\n",
    "            \n",
    "            date_obj = datetime(year, month, day)\n",
    "            return date_obj.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            return \"Invalide Date Format\"\n",
    "    return \"Date is not found!\"\n",
    "\n",
    "\n",
    "def scrape_article_content(article_number):\n",
    "    url = url_template.format(article_number)\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        if is_page_not_found(soup):\n",
    "            print(f\"文章 {article_number} 不存在（404 页面）。\")\n",
    "            return {\"url\": url, \"error\": \"404 Not Found\"}\n",
    "\n",
    "        # 提取日期信息\n",
    "        date_info = extract_date(soup)\n",
    "        date_details = extract_date_details(date_info) if date_info else None\n",
    "\n",
    "        # 找到文章主体内容\n",
    "        article_body = soup.find(\"div\", class_=\"article__body\")\n",
    "        content = []\n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "            content_list = [paragraph.get_text().strip() for paragraph in paragraphs]\n",
    "            content = \"\\n\".join(\n",
    "                paragraph.get_text().strip() for paragraph in paragraphs\n",
    "            )\n",
    "\n",
    "        # 返回结果\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"date_info\": date_details,\n",
    "            \"content\": content,\n",
    "            \"content_list\": content_list,\n",
    "            \"error\": \"NONE\",\n",
    "        }\n",
    "    else:\n",
    "        # print(f\"无法获取文章 {article_number}，状态码: {response.status_code}\")\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"date_info\": \"\",\n",
    "            \"content\": \"\",\n",
    "            \"content_list\": \"\",\n",
    "            \"error\": f\"HTTP {response.status_code}\",\n",
    "        }\n",
    "\n",
    "\n",
    "# 2160001 - > 23.01.2024\n",
    "# 2260902 - > 20.12.2024\n",
    "# 示例：提取文章内容\n",
    "scrape_article_content(2260902)\n",
    "# scrape_article_content(2260802)\n",
    "# 如果需要处理多个文章，可以用循环\n",
    "csv_file = \"articles.csv\"\n",
    "\n",
    "# 如果文件不存在，先创建并写入表头\n",
    "if not os.path.exists(csv_file):\n",
    "    columns = [\"url\", \"date_info\", \"content\", \"content_list\", \"error\"]\n",
    "    pd.DataFrame(columns=columns).to_csv(csv_file, index=False)\n",
    "\n",
    "# 遍历所有文章编号\n",
    "for article_number in tqdm(range(2160001, 2260903), desc=\"Processing articles\"):\n",
    "    response_dict = scrape_article_content(article_number)\n",
    "\n",
    "    # 将字典转换为 DataFrame\n",
    "    df = pd.DataFrame([response_dict])\n",
    "\n",
    "    # 逐条追加到 CSV 文件中，不写入索引和表头\n",
    "    df.to_csv(csv_file, mode=\"a\", header=False, index=False)\n",
    "\n",
    "    # 每次请求后暂停 5 秒\n",
    "    time.sleep(1)\n",
    "    # print(f\"Saved article {article_number} to CSV.\")\n",
    "print(\"Data has been saved to 'articles.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env_lujun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
