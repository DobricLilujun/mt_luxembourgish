{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Testing For Training Model Testing\n",
    "\n",
    "This paper needs to support two models: NLLB and LLAMA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLAMA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import torch\n",
    "\n",
    "import pandas as pds\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "from sacrebleu.metrics import CHRF\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "MAX_LEN = 512\n",
    "model_path = \"/home/snt/llm_models/Llama-3.2-3B-Instruct\"\n",
    "val_dataset_path = \"data/training_dataset/dataset_nllb_split.jsonl\"\n",
    "flore_dataset_path = \"data/fake_targets/flores_devtest_arrow\"\n",
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime('%m_%d_%H_%M')\n",
    "eval_output_path = val_dataset_path.split(\"/\")[-1].replace(\".jsonl\", f\"_{formatted_time}_eval_from_Llama3-3B.jsonl\")\n",
    "sample_num = None  # Number of samples to evaluate otherwise set to None\n",
    "\n",
    "\n",
    "# src_lng = \"English\"\n",
    "# src_lng_abr = \"sentence_eng_Latn\"\n",
    "\n",
    "src_lng = \"Luxembourgish\"\n",
    "src_lng_abr = \"sentence_ltz_Latn\"\n",
    "\n",
    "# tgt_lng = \"Luxembourgish\"\n",
    "# tgt_lng_abr = \"sentence_ltz_Latn\"\n",
    "\n",
    "tgt_lng = \"English\"\n",
    "tgt_lng_abr = \"sentence_eng_Latn\"\n",
    "\n",
    "device=\"cuda:0\"\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Load dataset\n",
    "if val_dataset_path.endswith(\".jsonl\"):\n",
    "    dataset = Dataset.from_json(val_dataset_path)  # Ensure correct format\n",
    "else:\n",
    "    dataset = load_from_disk(val_dataset_path)\n",
    "\n",
    "# Filter by split\n",
    "if sample_num:\n",
    "    val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\").select(range(sample_num))\n",
    "else:\n",
    "    val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})  # This pair cannot be changed\n",
    "\n",
    "if sample_num:\n",
    "    val_flores_dataset = (\n",
    "        load_from_disk(flore_dataset_path)\n",
    "        .rename_columns(\n",
    "            {\n",
    "                tgt_lng_abr: tgt_lng,\n",
    "                src_lng_abr: src_lng,\n",
    "            }\n",
    "        )\n",
    "        .select([i for i in range(10)])\n",
    "    )\n",
    "else:\n",
    "    val_flores_dataset = load_from_disk(flore_dataset_path).rename_columns(\n",
    "        {\n",
    "            tgt_lng_abr: tgt_lng,\n",
    "            src_lng_abr: src_lng,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights (was previously converted to 4 bits)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "# Function to generate from the model\n",
    "def generate_response(prompt, model):\n",
    "    encoded_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    model_inputs = encoded_input.to(device)\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=MAX_LEN * 2,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "    return decoded_output[0].replace(prompt, \"\")\n",
    "\n",
    "def compute_jaccard(prediction: str, reference: str) -> float:\n",
    "    pred_set = set(prediction.split())\n",
    "    ref_set = set(reference.split())\n",
    "    if not pred_set and not ref_set:\n",
    "        return 1.0 \n",
    "    return len(pred_set & ref_set) / len(pred_set | ref_set)\n",
    "\n",
    "\n",
    "def create_prompt(\n",
    "    sample, src_lng, tgt_lng, mode=\"train\", tokenizer=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a prompt using the model's EOS token.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A dictionary containing source and target text.\n",
    "        mode (str): The mode, either 'train' or 'test'.\n",
    "        src_lng (str): Source language name.\n",
    "        tgt_lng (str): Target language name.\n",
    "        tokenizer: The tokenizer associated with the model (required to fetch EOS token).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the constructed prompt.\n",
    "    \"\"\"\n",
    "    # Validate the tokenizer input\n",
    "    if tokenizer is None or tokenizer.eos_token is None:\n",
    "        raise ValueError(\"A tokenizer with a defined EOS token is required.\")\n",
    "\n",
    "    # Define the system message template.\n",
    "    system_message = f\"Translate the {src_lng} input text into {tgt_lng}.\".upper()\n",
    "    input_text = sample[src_lng.capitalize()].strip()  # Extract the input text.\n",
    "    response = (\n",
    "        sample[tgt_lng.capitalize()].strip() if tgt_lng.capitalize() in sample else \"\"\n",
    "    )  # Extract the target text.\n",
    "\n",
    "    # Get the EOS token from the tokenizer.\n",
    "    eos_token = tokenizer.eos_token\n",
    "\n",
    "    # Construct the full prompt.\n",
    "    full_prompt = (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        + system_message\n",
    "        + \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "    full_prompt += (\n",
    "        input_text + \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "    if mode == \"train\":\n",
    "        full_prompt += response + eos_token\n",
    "    return {\"prompt_response\": full_prompt}\n",
    "\n",
    "\n",
    "def generate_dataset_responses(dataset, model, src_lng, tgt_lng, tokenizer=None):\n",
    "    \"\"\"Generates prompts and corresponding LLM responses for the \"test\" split of a dataset,\n",
    "    and computes the SPBLEU score by comparing the LLM responses to the ground truth.\"\"\"\n",
    "\n",
    "    source = []\n",
    "    predictions = []  # List to store ground truth responses\n",
    "    targets = []  # List to store LLM generated responses\n",
    "    index_uniques = []\n",
    "\n",
    "    for sample in tqdm(dataset, desc=\"Generating responses\"):\n",
    "        input_prompt = create_prompt(\n",
    "            sample, mode=\"test\", src_lng=src_lng, tgt_lng=tgt_lng, tokenizer=tokenizer\n",
    "        )[\n",
    "            \"prompt_response\"\n",
    "        ]  # Create the prompt in \"test\" mode\n",
    "\n",
    "        llm_response = (\n",
    "            generate_response(input_prompt, model)\n",
    "            .replace(\"<|begin_of_text|>\", \"\")\n",
    "            .replace(\"<|eot_id|>\", \"\")\n",
    "        )\n",
    "\n",
    "        ground_truth = sample.get(tgt_lng, \"\")  # Get the ground truth (adjust field name as needed)\n",
    "        index_unique = sample.get(\"index_unique\", \"\")\n",
    "        # Append the LLM response and ground truth\n",
    "\n",
    "        predictions.append(llm_response)\n",
    "        targets.append([ground_truth])  # References should be in list format for SPBLEU\n",
    "        source.append(sample.get(src_lng, \"\"))\n",
    "        index_uniques.append(index_unique)\n",
    "\n",
    "    df_results = pd.DataFrame(\n",
    "        list(zip(source, predictions, targets, index_uniques)),\n",
    "        columns=[\"LLM_Input\", \"LLM_Output\", \"Ground_Truth\", \"index_unique\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    ## SPBLEU Score\n",
    "    spbleu_scores = [\n",
    "        sacrebleu.corpus_bleu([p], [t], tokenize=\"flores200\").score\n",
    "        for p, t in zip(predictions, targets)\n",
    "    ]\n",
    "    df_results[\"SPBLEU_Score\"] = spbleu_scores\n",
    "\n",
    "    ## CharF++\n",
    "    chrf_metric = CHRF(word_order=3) \n",
    "    charf_scores = [chrf_metric.sentence_score(p, t).score for p, t in zip(predictions, targets)]\n",
    "\n",
    "    ## Jaccard Score\n",
    "    jaccard_scores = [\n",
    "        compute_jaccard(p, t[0]) for p, t in zip(predictions, targets)\n",
    "    ]\n",
    "    df_results[\"CharF++_Score\"] = charf_scores\n",
    "    df_results[\"Jaccard_Score\"] = jaccard_scores\n",
    "    \n",
    "\n",
    "    ## Average Scores\n",
    "    average_charf = df_results[\"CharF++_Score\"].mean()\n",
    "    average_jaccard = df_results[\"Jaccard_Score\"].mean()\n",
    "    average_spbleu = df_results[\"SPBLEU_Score\"].mean()\n",
    "\n",
    "    print(f\"Average SPBLEU Score: {average_spbleu:.2f}\")\n",
    "    print(f\"Average CharF++ Score: {average_charf:.2f}\")\n",
    "    print(f\"Average Jaccard Score: {average_jaccard:.2f}\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "print (\"Validation RTL Results\")\n",
    "print (\"----------------------\")\n",
    "df_RTL_results = generate_dataset_responses(\n",
    "    dataset=val_dataset, model=model, src_lng = src_lng, tgt_lng=tgt_lng, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "df_RTL_results[\"Dataset\"] = \"RTL\"\n",
    "\n",
    "print (\"FLORES 200 Results\")\n",
    "print (\"----------------------\")\n",
    "\n",
    "df_flores_results = generate_dataset_responses(\n",
    "    dataset=val_flores_dataset, model=model, src_lng = src_lng, tgt_lng=tgt_lng, tokenizer=tokenizer\n",
    ")\n",
    "df_flores_results[\"Dataset\"] = \"FLORES\"\n",
    "\n",
    "df_results = pd.concat([df_RTL_results, df_flores_results], axis=0)\n",
    "df_results.to_json(eval_output_path, orient=\"records\", lines=True)\n",
    "print(f\"Results saved to {eval_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_lux_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
