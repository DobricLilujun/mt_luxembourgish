{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Testing For Training Model Testing\n",
    "\n",
    "This paper needs to support two models: NLLB and LLAMA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLAMA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RTL Results\n",
      "----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 300/300 [46:25<00:00,  9.29s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SPBLEU Score: 5.98\n",
      "Average CharF++ Score: 25.88\n",
      "Average Jaccard Score: 0.11\n",
      "FLORES 200 Results\n",
      "----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses:   1%|          | 10/1012 [00:43<1:12:02,  4.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 230\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLORES 200 Results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 230\u001b[0m df_flores_results \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dataset_responses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_flores_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_lng\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msrc_lng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_lng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_lng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m df_flores_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLORES\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_RTL_results, df_flores_results], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 173\u001b[0m, in \u001b[0;36mgenerate_dataset_responses\u001b[0;34m(dataset, model, src_lng, tgt_lng, tokenizer)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating responses\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    168\u001b[0m     input_prompt \u001b[38;5;241m=\u001b[39m create_prompt(\n\u001b[1;32m    169\u001b[0m         sample, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, src_lng\u001b[38;5;241m=\u001b[39msrc_lng, tgt_lng\u001b[38;5;241m=\u001b[39mtgt_lng, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m    170\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_response\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    172\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 173\u001b[0m         \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|begin_of_text|>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     ground_truth \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mget(tgt_lng, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Get the ground truth (adjust field name as needed)\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     index_unique \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_unique\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 102\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m    100\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m encoded_input\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 102\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m decoded_output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/transformers/generation/utils.py:3223\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3220\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m next_token_logits\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/transformers/generation/logits_process.py:78\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLogitsProcessorList\u001b[39;00m(\u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    This class can be used to create a list of [`LogitsProcessor`] to subsequently process a `scores` input tensor.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    This class inherits from list and adds a specific *__call__* method to apply each [`LogitsProcessor`] to the\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    inputs.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import torch\n",
    "\n",
    "import pandas as pds\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "from sacrebleu.metrics import CHRF\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "import json\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "MAX_LEN = 512\n",
    "model_path = \"/home/snt/llm_models/Llama-3.2-3B-Instruct\"\n",
    "val_dataset_path = \"data/training_dataset/dataset_val_300.jsonl\"\n",
    "flore_dataset_path = \"data/fake_targets/flores_devtest_arrow\"\n",
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime('%m_%d_%H_%M')\n",
    "eval_output_path = val_dataset_path.split(\"/\")[-1].replace(\".jsonl\", f\"_{formatted_time}_eval_from_Llama3-3B.jsonl\")\n",
    "sample_num = None  # Number of samples to evaluate otherwise set to None\n",
    "\n",
    "\n",
    "src_lng = \"English\"\n",
    "src_lng_abr = \"sentence_eng_Latn\"\n",
    "\n",
    "# src_lng = \"Luxembourgish\"\n",
    "# src_lng_abr = \"sentence_ltz_Latn\"\n",
    "\n",
    "tgt_lng = \"Luxembourgish\"\n",
    "tgt_lng_abr = \"sentence_ltz_Latn\"\n",
    "\n",
    "# tgt_lng = \"English\"\n",
    "# tgt_lng_abr = \"sentence_eng_Latn\"\n",
    "\n",
    "device=\"cuda:0\"\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Load dataset\n",
    "if val_dataset_path.endswith(\".jsonl\"):\n",
    "    dataset = Dataset.from_json(val_dataset_path)  # Ensure correct format\n",
    "else:\n",
    "    dataset = load_from_disk(val_dataset_path)\n",
    "\n",
    "# Filter by split\n",
    "if sample_num:\n",
    "    val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\").select(range(sample_num))\n",
    "else:\n",
    "    val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})  # This pair cannot be changed\n",
    "\n",
    "if sample_num:\n",
    "    val_flores_dataset = (\n",
    "        load_from_disk(flore_dataset_path)\n",
    "        .rename_columns(\n",
    "            {\n",
    "                tgt_lng_abr: tgt_lng,\n",
    "                src_lng_abr: src_lng,\n",
    "            }\n",
    "        )\n",
    "        .select([i for i in range(10)])\n",
    "    )\n",
    "else:\n",
    "    val_flores_dataset = load_from_disk(flore_dataset_path).rename_columns(\n",
    "        {\n",
    "            tgt_lng_abr: tgt_lng,\n",
    "            src_lng_abr: src_lng,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights (was previously converted to 4 bits)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "# Function to generate from the model\n",
    "def generate_response(prompt, model):\n",
    "    encoded_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    model_inputs = encoded_input.to(device)\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=MAX_LEN * 2,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "    return decoded_output[0].replace(prompt, \"\")\n",
    "\n",
    "def compute_jaccard(prediction: str, reference: str) -> float:\n",
    "    pred_set = set(prediction.split())\n",
    "    ref_set = set(reference.split())\n",
    "    if not pred_set and not ref_set:\n",
    "        return 1.0 \n",
    "    return len(pred_set & ref_set) / len(pred_set | ref_set)\n",
    "\n",
    "\n",
    "def create_prompt(\n",
    "    sample, src_lng, tgt_lng, mode=\"train\", tokenizer=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a prompt using the model's EOS token.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A dictionary containing source and target text.\n",
    "        mode (str): The mode, either 'train' or 'test'.\n",
    "        src_lng (str): Source language name.\n",
    "        tgt_lng (str): Target language name.\n",
    "        tokenizer: The tokenizer associated with the model (required to fetch EOS token).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the constructed prompt.\n",
    "    \"\"\"\n",
    "    # Validate the tokenizer input\n",
    "    if tokenizer is None or tokenizer.eos_token is None:\n",
    "        raise ValueError(\"A tokenizer with a defined EOS token is required.\")\n",
    "\n",
    "    # Define the system message template.\n",
    "    system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "    input_text = sample[src_lng.capitalize()].strip()  # Extract the input text.\n",
    "    response = (\n",
    "        sample[tgt_lng.capitalize()].strip() if tgt_lng.capitalize() in sample else \"\"\n",
    "    )  # Extract the target text.\n",
    "\n",
    "    # Get the EOS token from the tokenizer.\n",
    "    eos_token = tokenizer.eos_token\n",
    "\n",
    "    # Construct the full prompt.\n",
    "    full_prompt = (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\" + system_message + \"<|eot_id|>\" +  # System Message\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + f\"Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\\n\\n {input_text}.\"  +  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # User Query\n",
    "    )\n",
    "    if mode == \"train\":\n",
    "        full_prompt += (response + eos_token)  # Responses\n",
    "\n",
    "    return {\"prompt_response\": full_prompt}\n",
    "\n",
    "def generate_dataset_responses(dataset, model, src_lng, tgt_lng,tokenizer=None):\n",
    "    \"\"\"Generates prompts and corresponding LLM responses for the \"test\" split of a dataset,\n",
    "    and computes the SPBLEU score by comparing the LLM responses to the ground truth.\"\"\"\n",
    "    df_results = pd.DataFrame()\n",
    "    for sample in tqdm(dataset, desc=\"Generating responses\"):\n",
    "        input_prompt = create_prompt(\n",
    "            sample, mode=\"test\", src_lng=src_lng, tgt_lng=tgt_lng, tokenizer=tokenizer\n",
    "        )[\"prompt_response\"]\n",
    "\n",
    "        llm_response = (\n",
    "            generate_response(input_prompt, model)\n",
    "            .replace(\"<|begin_of_text|>\", \"\")\n",
    "            .replace(\"<|eot_id|>\", \"\")\n",
    "        )\n",
    "\n",
    "        ground_truth = sample.get(tgt_lng, \"\")  # Get the ground truth (adjust field name as needed)\n",
    "        index_unique = sample.get(\"index_unique\", \"\")\n",
    "\n",
    "        ## Compute Scores\n",
    "        spbleu_score = sacrebleu.corpus_bleu([llm_response], [[ground_truth]], tokenize=\"flores200\").score\n",
    "\n",
    "        chrf_metric = CHRF(word_order=3)\n",
    "        charf_score = chrf_metric.sentence_score(llm_response, [ground_truth]).score\n",
    "\n",
    "        jaccard_score = compute_jaccard(llm_response, ground_truth)\n",
    "\n",
    "        ## Create JSON entry\n",
    "        result = {\n",
    "            \"LLM_Input\": input_prompt,\n",
    "            \"LLM_Output\": llm_response,\n",
    "            \"Ground_Truth\": ground_truth,\n",
    "            \"index_unique\": index_unique,\n",
    "            \"SPBLEU_Score\": spbleu_score,\n",
    "            \"CharF++_Score\": charf_score,\n",
    "            \"Jaccard_Score\": jaccard_score,\n",
    "        }\n",
    "        updated_dataframe = pd.DataFrame([result])\n",
    "        updated_dataframe.to_json(\n",
    "                eval_output_path,\n",
    "                orient=\"records\",\n",
    "                lines=True,\n",
    "                mode=\"a\",\n",
    "        )\n",
    "        df_results = pd.concat([df_results, updated_dataframe], axis=0)\n",
    "\n",
    "    ## Average Scores\n",
    "    average_charf = df_results[\"CharF++_Score\"].mean()\n",
    "    average_jaccard = df_results[\"Jaccard_Score\"].mean()\n",
    "    average_spbleu = df_results[\"SPBLEU_Score\"].mean()\n",
    "\n",
    "    print(f\"Average SPBLEU Score: {average_spbleu:.2f}\")\n",
    "    print(f\"Average CharF++ Score: {average_charf:.2f}\")\n",
    "    print(f\"Average Jaccard Score: {average_jaccard:.2f}\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "print (\"Validation RTL Results\")\n",
    "print (\"----------------------\")\n",
    "df_RTL_results = generate_dataset_responses(\n",
    "    dataset=val_dataset, model=model, src_lng = src_lng, tgt_lng=tgt_lng, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "df_RTL_results[\"Dataset\"] = \"RTL\"\n",
    "\n",
    "print (\"FLORES 200 Results\")\n",
    "print (\"----------------------\")\n",
    "\n",
    "df_flores_results = generate_dataset_responses(\n",
    "    dataset=val_flores_dataset, model=model, src_lng = src_lng, tgt_lng=tgt_lng, tokenizer=tokenizer\n",
    ")\n",
    "df_flores_results[\"Dataset\"] = \"FLORES\"\n",
    "\n",
    "df_results = pd.concat([df_RTL_results, df_flores_results], axis=0)\n",
    "x\n",
    "print(f\"Results saved to {eval_output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_lux_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
