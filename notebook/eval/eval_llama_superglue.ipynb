{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Testing For Training Model Testing\n",
    "\n",
    "This paper needs to support two models: NLLB and LLAMA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "import json\n",
    "############################################################################################################\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_prediction_with_conversion(task, example, prediction):\n",
    "    # Helper functions for conversion\n",
    "    def bool_to_binary(value):\n",
    "        # Strip whitespace and convert to lowercase for consistency\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the string is exactly \"true\"\n",
    "        if value == \"true\":\n",
    "            return 1\n",
    "        elif value == \"false\":\n",
    "            return 0\n",
    "\n",
    "        # Check if the string contains \"true\" but does not contain \"false\"\n",
    "        elif \"true\" in value and \"false\" not in value:\n",
    "            return 1\n",
    "\n",
    "        # Check if the string contains \"false\" but does not contain \"true\"\n",
    "        elif \"false\" in value and \"true\" not in value:\n",
    "            return 0\n",
    "\n",
    "        # If both \"true\" and \"false\" are present or neither is present, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def yes_no_to_binary(value):\n",
    "        # Strip whitespace and convert to lowercase for consistency\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the string is exactly \"yes\"\n",
    "        if value == \"yes\":\n",
    "            return 1\n",
    "        elif value == \"no\":\n",
    "            return 0\n",
    "\n",
    "        # Check if the string contains \"yes\" but does not contain \"no\"\n",
    "        elif \"yes\" in value and \"no\" not in value:\n",
    "            return 1\n",
    "\n",
    "        # Check if the string contains \"no\" but does not contain \"yes\"\n",
    "        elif \"no\" in value and \"yes\" not in value:\n",
    "            return 0\n",
    "\n",
    "        # If both \"yes\" and \"no\" are present or neither is present, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def entailment_to_label(value):\n",
    "        # Define the mapping for entailment, contradiction, and neutral\n",
    "        mapping = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "\n",
    "        # Normalize the input by stripping whitespace and converting to lowercase\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the input matches exactly one of the keys in the mapping\n",
    "        if value in mapping:\n",
    "            return mapping[value]\n",
    "\n",
    "        # Check if the input contains one of the keys without ambiguity\n",
    "        elif (\n",
    "            \"entailment\" in value\n",
    "            and \"contradiction\" not in value\n",
    "            and \"neutral\" not in value\n",
    "        ):\n",
    "            return mapping[\"entailment\"]\n",
    "        elif (\n",
    "            \"contradiction\" in value\n",
    "            and \"entailment\" not in value\n",
    "            and \"neutral\" not in value\n",
    "        ):\n",
    "            return mapping[\"contradiction\"]\n",
    "        elif (\n",
    "            \"neutral\" in value\n",
    "            and \"entailment\" not in value\n",
    "            and \"contradiction\" not in value\n",
    "        ):\n",
    "            return mapping[\"neutral\"]\n",
    "\n",
    "        # If the input is ambiguous or invalid, return -1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def choice_to_binary(value):\n",
    "        # Normalize the input by stripping whitespace and converting to lowercase\n",
    "        value = value.strip().lower()\n",
    "\n",
    "        # Check if the input contains 'choice 1' and does not contain 'choice 2'\n",
    "        if \"choice 1\" in value and \"choice 2\" not in value:\n",
    "            return 0\n",
    "        elif \"choice 2\" in value:\n",
    "            return 1\n",
    "\n",
    "        # If the input does not match any of the conditions, return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Task-specific evaluation\n",
    "    if task == \"boolq\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return bool_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"cb\":\n",
    "        # Convert prediction (entailment/contradiction/neutral) to label (0/1/2)\n",
    "        return entailment_to_label(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"copa\":\n",
    "        # Convert prediction (choice1/choice2) to binary and compare with label (0/1)\n",
    "        return choice_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"multirc\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return bool_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"record\":\n",
    "        # Direct comparison of prediction with the correct entity\n",
    "        processed_answers = [answer.strip().lower() for answer in example[\"answers\"]]\n",
    "        return prediction.strip().lower() in processed_answers\n",
    "\n",
    "    elif task == \"rte\":\n",
    "        # Convert prediction (Yes/No) to binary and compare with label (1/0)\n",
    "        return yes_no_to_binary(prediction) == (1 - int(example[\"label\"]))\n",
    "\n",
    "    elif task == \"wic\":\n",
    "        # Convert prediction (Yes/No) to binary and compare with label (0/1)\n",
    "        return yes_no_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    elif task == \"wsc\":\n",
    "        # Convert prediction (True/False) to binary and compare with label (0/1)\n",
    "        return yes_no_to_binary(prediction) == int(example[\"label\"])\n",
    "\n",
    "    # Default case: unknown task\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]\n",
      "/tmp/ipykernel_326653/3831194500.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_df = val_df.groupby(\"task\").apply(lambda x: x.sample(n=5, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "model_path = \"/home/snt/llm_models/gemma-2-2b-it\"\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "val_dataset_path = \"/home/snt/projects_lujun/mt_luxembourgish/data/super_glue_data.jsonl\"\n",
    "flore_dataset_path = \"data/fake_targets/flores_devtest_arrow\"\n",
    "output_path = f\"data/results_test_{model_name}.jsonl\"\n",
    "\n",
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime('%m_%d_%H_%M')\n",
    "eval_output_path = val_dataset_path.split(\"/\")[-1].replace(\".jsonl\", f\"_{formatted_time}_eval_from_Llama3-3B.jsonl\")\n",
    "sample_num = None  # Number of samples to evaluate otherwise set to None\n",
    "device=\"cuda:0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights (was previously converted to 4 bits)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "# Function to generate from the model\n",
    "def generate_response(prompt, model):\n",
    "    encoded_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    model_inputs = encoded_input.to(device)\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=MAX_LEN,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "    return decoded_output[0].replace(prompt, \"\")\n",
    "\n",
    "df = pd.read_json(val_dataset_path, lines=True)\n",
    "val_df = df[df[\"dataset_label\"]==\"validation\"]\n",
    "# val_df = val_df.groupby(\"task\").apply(lambda x: x.sample(n=5, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "for index, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Processing\", ncols=80):\n",
    "    prompt = row[\"prompt\"]\n",
    "    system_message = f\"You are a helpful AI assistant.\"\n",
    "\n",
    "    if \"gemma\" in model_name:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    \n",
    "    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    prediction = generate_response(\n",
    "        full_prompt,\n",
    "        model,\n",
    "    )\n",
    "    \n",
    "    label = evaluate_prediction_with_conversion(row[\"task\"], row[\"example\"], prediction)\n",
    "    result_df = pd.DataFrame(\n",
    "        {\n",
    "            \"task\": row[\"task\"],\n",
    "            \"example\": json.dumps(row[\"example\"]),\n",
    "            \"prompt\": full_prompt,\n",
    "            \"prediction\": prediction,\n",
    "            \"label\": label,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    result_df.to_json(\n",
    "        output_path,\n",
    "        orient=\"records\",\n",
    "        lines=True,\n",
    "        mode=\"a\",\n",
    "    )\n",
    "\n",
    "\n",
    "result_df = pd.read_json(output_path, lines=True)\n",
    "task_accuracy = result_df.groupby(\"task\")[\"label\"].mean() * 100\n",
    "for task, acc in task_accuracy.items():\n",
    "    print(f\"Task: {task}, Accuracy: {acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_lux_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
