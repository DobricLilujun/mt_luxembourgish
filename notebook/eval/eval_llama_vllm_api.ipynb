{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "from datasets import Dataset, load_from_disk\n",
    "from sacrebleu.metrics import CHRF\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "VLLM_API_URL = \"http://localhost:8000/generate\"\n",
    "MAX_LEN = 512\n",
    "val_dataset_path = \"data/training_dataset/dataset_val_300.jsonl\"\n",
    "flore_dataset_path = \"data/fake_targets/flores_devtest_arrow\"\n",
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime('%m_%d_%H_%M')\n",
    "eval_output_path = val_dataset_path.split(\"/\")[-1].replace(\".jsonl\", f\"_{formatted_time}_eval_from_vLLM.jsonl\")\n",
    "sample_num = None  # Number of samples to evaluate otherwise set to None\n",
    "\n",
    "src_lng = \"English\"\n",
    "src_lng_abr = \"sentence_eng_Latn\"\n",
    "\n",
    "tgt_lng = \"Luxembourgish\"\n",
    "tgt_lng_abr = \"sentence_ltz_Latn\"\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Load dataset\n",
    "if val_dataset_path.endswith(\".jsonl\"):\n",
    "    dataset = Dataset.from_json(val_dataset_path)\n",
    "else:\n",
    "    dataset = load_from_disk(val_dataset_path)\n",
    "\n",
    "if sample_num:\n",
    "    val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\").select(range(sample_num))\n",
    "else:\n",
    "    val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "if sample_num:\n",
    "    val_flores_dataset = (\n",
    "        load_from_disk(flore_dataset_path)\n",
    "        .rename_columns({tgt_lng_abr: tgt_lng, src_lng_abr: src_lng})\n",
    "        .select([i for i in range(10)])\n",
    "    )\n",
    "else:\n",
    "    val_flores_dataset = load_from_disk(flore_dataset_path).rename_columns({tgt_lng_abr: tgt_lng, src_lng_abr: src_lng})\n",
    "\n",
    "\n",
    "def call_vllm(prompt):\n",
    "    response = requests.post(VLLM_API_URL, json={\"prompt\": prompt, \"max_tokens\": MAX_LEN * 2, \"temperature\": 1.0})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"generated_text\"]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def compute_jaccard(prediction: str, reference: str) -> float:\n",
    "    pred_set = set(prediction.split())\n",
    "    ref_set = set(reference.split())\n",
    "    if not pred_set and not ref_set:\n",
    "        return 1.0\n",
    "    return len(pred_set & ref_set) / len(pred_set | ref_set)\n",
    "\n",
    "\n",
    "def create_prompt(sample, src_lng, tgt_lng):\n",
    "    system_message = \"You are a helpful AI assistant for translation.\"\n",
    "    input_text = sample[src_lng.capitalize()].strip()\n",
    "    full_prompt = f\"{system_message}\\n\\nTranslate the {src_lng} input text into {tgt_lng}.\\n\\n{input_text}\"\n",
    "    return full_prompt\n",
    "\n",
    "\n",
    "def generate_dataset_responses(dataset, src_lng, tgt_lng):\n",
    "    df_results = pd.DataFrame()\n",
    "    for sample in tqdm(dataset, desc=\"Generating responses\"):\n",
    "        input_prompt = create_prompt(sample, src_lng, tgt_lng)\n",
    "        llm_response = call_vllm(input_prompt).strip()\n",
    "        ground_truth = sample.get(tgt_lng, \"\")\n",
    "        index_unique = sample.get(\"index_unique\", \"\")\n",
    "        \n",
    "        spbleu_score = sacrebleu.corpus_bleu([llm_response], [[ground_truth]], tokenize=\"flores200\").score\n",
    "        chrf_metric = CHRF(word_order=3)\n",
    "        charf_score = chrf_metric.sentence_score(llm_response, [ground_truth]).score\n",
    "        jaccard_score = compute_jaccard(llm_response, ground_truth)\n",
    "\n",
    "        result = {\n",
    "            \"LLM_Input\": input_prompt,\n",
    "            \"LLM_Output\": llm_response,\n",
    "            \"Ground_Truth\": ground_truth,\n",
    "            \"index_unique\": index_unique,\n",
    "            \"SPBLEU_Score\": spbleu_score,\n",
    "            \"CharF++_Score\": charf_score,\n",
    "            \"Jaccard_Score\": jaccard_score,\n",
    "        }\n",
    "        updated_dataframe = pd.DataFrame([result])\n",
    "        updated_dataframe.to_json(eval_output_path, orient=\"records\", lines=True, mode=\"a\")\n",
    "        df_results = pd.concat([df_results, updated_dataframe], axis=0)\n",
    "\n",
    "    print(f\"Average SPBLEU Score: {df_results['SPBLEU_Score'].mean():.2f}\")\n",
    "    print(f\"Average CharF++ Score: {df_results['CharF++_Score'].mean():.2f}\")\n",
    "    print(f\"Average Jaccard Score: {df_results['Jaccard_Score'].mean():.2f}\")\n",
    "    return df_results\n",
    "\n",
    "print(\"Validation RTL Results\")\n",
    "df_RTL_results = generate_dataset_responses(val_dataset, src_lng, tgt_lng)\n",
    "df_RTL_results[\"Dataset\"] = \"RTL\"\n",
    "\n",
    "print(\"FLORES 200 Results\")\n",
    "df_flores_results = generate_dataset_responses(val_flores_dataset, src_lng, tgt_lng)\n",
    "df_flores_results[\"Dataset\"] = \"FLORES\"\n",
    "\n",
    "df_results = pd.concat([df_RTL_results, df_flores_results], axis=0)\n",
    "print(f\"Results saved to {eval_output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_lux_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
