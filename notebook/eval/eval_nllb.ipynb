{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Testing For Training Model Testing\n",
    "\n",
    "This paper needs to support two models: NLLB and LLAMA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RTL Results\n",
      "----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating...:   3%|▎         | 10/300 [00:20<08:48,  1.82s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Translating...: 100%|██████████| 300/300 [16:31<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SPBLEU Score: 19.97\n",
      "Average CharF++ Score: 37.06\n",
      "Average Jaccard Score: 0.27\n",
      "FLORES 200 Results\n",
      "----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating...:  54%|█████▍    | 550/1012 [17:09<7:07:01, 55.46s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pds\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "from sacrebleu.metrics import CHRF\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "\n",
    "############################################################################################################\n",
    "MAX_LEN = 512\n",
    "model_path = \"/home/snt/llm_models/nllb-200-3.3B\" \n",
    "val_dataset_path = \"data/training_dataset/dataset_val_300.jsonl\"\n",
    "flore_dataset_path = \"data/fake_targets/flores_devtest_arrow\"\n",
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime('%m_%d_%H_%M')\n",
    "eval_output_path = val_dataset_path.split(\"/\")[-1].replace(\".jsonl\", f\"_{formatted_time}_eval_from_nllb.jsonl\")\n",
    "sample_num = None  # Number of samples to evaluate， otherwise set to None if you want to evaluate the whole dataset\n",
    "device_map=\"cuda:0\"\n",
    "\n",
    "src_lng = \"English\"\n",
    "src_lng_abr = \"sentence_eng_Latn\"\n",
    "\n",
    "# src_lng = \"Luxembourgish\"\n",
    "# src_lng_abr = \"sentence_ltz_Latn\"\n",
    "\n",
    "tgt_lng = \"Luxembourgish\"\n",
    "tgt_lng_abr = \"sentence_ltz_Latn\"\n",
    "\n",
    "# tgt_lng = \"English\"\n",
    "# tgt_lng_abr = \"sentence_eng_Latn\"\n",
    "device=\"cuda:0\"\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Load dataset\n",
    "if val_dataset_path.endswith(\".jsonl\"):\n",
    "    dataset = Dataset.from_json(val_dataset_path)  # Ensure correct format\n",
    "else:\n",
    "    dataset = load_from_disk(val_dataset_path)\n",
    "\n",
    "# Filter by split\n",
    "if sample_num:\n",
    "    val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\").select(range(sample_num))\n",
    "else:\n",
    "    val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})  # This pair cannot be changed\n",
    "\n",
    "if sample_num:\n",
    "    val_flores_dataset = (\n",
    "        load_from_disk(flore_dataset_path)\n",
    "        .rename_columns(\n",
    "            {\n",
    "                tgt_lng_abr: tgt_lng,\n",
    "                src_lng_abr: src_lng,\n",
    "            }\n",
    "        )\n",
    "        .select([i for i in range(10)])\n",
    "    )\n",
    "else:\n",
    "    val_flores_dataset = load_from_disk(flore_dataset_path).rename_columns(\n",
    "        {\n",
    "            tgt_lng_abr: tgt_lng,\n",
    "            src_lng_abr: src_lng,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=src_lng_abr[9:], tgt_lang=tgt_lng_abr[9:], max_length = MAX_LEN *2, device=device)\n",
    "\n",
    "\n",
    "def compute_jaccard(prediction, reference):\n",
    "    pred_set = set(prediction.split())\n",
    "    ref_set = set(reference.split())\n",
    "    if not pred_set and not ref_set:\n",
    "        return 1.0\n",
    "    return len(pred_set & ref_set) / len(pred_set | ref_set)\n",
    "\n",
    "def generate_dataset_responses(dataset):\n",
    "    df_results = pd.DataFrame()\n",
    "    for sample in tqdm(dataset, desc=\"Translating...\"):\n",
    "        source_text = sample[src_lng.capitalize()].strip()\n",
    "        target_text = ( sample[tgt_lng.capitalize()].strip() if tgt_lng.capitalize() in sample else \"\") \n",
    "        prediction = translator(source_text)[0][\"translation_text\"]\n",
    "        index_unique = sample.get(\"index_unique\", \"\")\n",
    "\n",
    "        ## Compute Scores\n",
    "        spbleu_score = sacrebleu.corpus_bleu([prediction], [[target_text]], tokenize=\"flores200\").score\n",
    "        chrf_metric = CHRF(word_order=3)\n",
    "        charf_score = chrf_metric.sentence_score(prediction, [target_text]).score\n",
    "        jaccard_score = compute_jaccard(prediction, target_text)\n",
    "\n",
    "        ## Create JSON entry\n",
    "        result = {\n",
    "            \"LLM_Input\": source_text,\n",
    "            \"LLM_Output\": prediction,\n",
    "            \"Ground_Truth\": target_text,\n",
    "            \"index_unique\": index_unique,\n",
    "            \"SPBLEU_Score\": spbleu_score,\n",
    "            \"CharF++_Score\": charf_score,\n",
    "            \"Jaccard_Score\": jaccard_score,\n",
    "        }\n",
    "        updated_dataframe = pd.DataFrame([result])\n",
    "        updated_dataframe.to_json(\n",
    "                eval_output_path,\n",
    "                orient=\"records\",\n",
    "                lines=True,\n",
    "                mode=\"a\",\n",
    "        )\n",
    "        df_results = pd.concat([df_results, updated_dataframe], axis=0)\n",
    "\n",
    "    ## Average Scores\n",
    "    average_charf = df_results[\"CharF++_Score\"].mean()\n",
    "    average_jaccard = df_results[\"Jaccard_Score\"].mean()\n",
    "    average_spbleu = df_results[\"SPBLEU_Score\"].mean()\n",
    "\n",
    "    print(f\"Average SPBLEU Score: {average_spbleu:.2f}\")\n",
    "    print(f\"Average CharF++ Score: {average_charf:.2f}\")\n",
    "    print(f\"Average Jaccard Score: {average_jaccard:.2f}\")\n",
    "    return df_results\n",
    "\n",
    "print (\"Validation RTL Results\")\n",
    "print (\"----------------------\")\n",
    "df_RTL_results = generate_dataset_responses(dataset=val_dataset)\n",
    "\n",
    "df_RTL_results[\"Dataset\"] = \"RTL\"\n",
    "\n",
    "print (\"FLORES 200 Results\")\n",
    "print (\"----------------------\")\n",
    "\n",
    "df_flores_results = generate_dataset_responses(dataset=val_flores_dataset)\n",
    "df_flores_results[\"Dataset\"] = \"FLORES\"\n",
    "\n",
    "df_results = pd.concat([df_RTL_results, df_flores_results], axis=0)\n",
    "df_results.to_json(eval_output_path, orient=\"records\", lines=True)\n",
    "print(f\"Results saved to {eval_output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_lux_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
