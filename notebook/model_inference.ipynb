{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Inference Using the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext langid\n",
    "!pip install langchain\n",
    "!pip install 'transformers[torch]'\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "!pip install ctranslate2\n",
    "!pip install torch\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLLB Running Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start From Index:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 4/4 [00:23<00:00,  5.96s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed. Results saved to translation_nllb__20241117_000411.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = \"/home/users/luli/project/mt_luxembourgish\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "def get_latest_file(pattern):\n",
    "    files = [f for f in os.listdir() if f.startswith(pattern)]\n",
    "    return max(files, key=os.path.getmtime) if files else None\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "def translate_batch(config, df):\n",
    "    # Initialize model and tokenizer\n",
    "    model_path = config[\"model_name\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    \n",
    "    translator = pipeline(\n",
    "        \"translation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        src_lang=config[\"src_lang\"],\n",
    "        tgt_lang=config[\"tgt_lang\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        device=config[\"device\"]\n",
    "    )\n",
    "\n",
    "    # Determine output file name \n",
    "    prefix = config[\"prefix\"]\n",
    "    latest_file = get_latest_file(prefix)\n",
    "    if latest_file and not bool(config.get(\"is_new_file\", False)):\n",
    "        output_file = latest_file\n",
    "    else:\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"{prefix}_{current_time}.csv\"\n",
    "\n",
    "    # Load data and check starting index\n",
    "    texts = df[config[\"text_column\"]].tolist()\n",
    "    start_idx = (\n",
    "        load_checkpoint(latest_file, df, config[\"text_column\"]) \n",
    "        if (latest_file and not bool(config.get(\"is_new_file\", False))) \n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    print(\"Start From Index: \", start_idx)\n",
    "    texts = df[config[\"text_column\"]].to_list()\n",
    "    \n",
    "    # Batch translation and saving\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    for i in tqdm(range(start_idx, len(texts), batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        translated_batch = translator(batch)\n",
    "        \n",
    "        for j, text in enumerate(batch):\n",
    "            updated_row = df.iloc[i + j].copy()\n",
    "            updated_row[\"translated_text\"] = translated_batch[j][\"translation_text\"]\n",
    "            updated_dataframe = pd.DataFrame([updated_row])\n",
    "            \n",
    "            mode = \"w\" if i == start_idx and j == 0 and start_idx == 0 else \"a\"\n",
    "            header = mode == \"w\"\n",
    "            updated_dataframe.to_csv(output_file, index=False, mode=mode, header=header)\n",
    "\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "\n",
    "# Unified configuration dictionary\n",
    "config = {\n",
    "    \"model_name\": \"/mnt/lscratch/users/luli/model/nllb-200-3.3B/\",\n",
    "    \"src_lang\": \"ltz_Latn\",\n",
    "    \"tgt_lang\": \"eng_Latn\",\n",
    "    \"device\": 'cuda:2' if torch.cuda.is_available() else 'cpu',\n",
    "    \"max_length\": 360,\n",
    "    \"batch_size\": 6,\n",
    "    \"text_column\": \"subsentence\",\n",
    "    \"prefix\": \"translation_nllb_\",\n",
    "    \"is_new_file\": False\n",
    "}\n",
    "\n",
    "input_file = \"NC_lux_subsentences_test.csv\"\n",
    "dataset_df = pd.read_csv(input_file)\n",
    "translate_batch(config, dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM running Local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/luli/.conda/envs/mt_lux_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start From Index:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 4/4 [06:17<00:00, 94.48s/batch] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed. Results saved to translation_LLM_huggingface_pipeline__20241117_003434.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "import sys\n",
    "\n",
    "project_root = \"/home/users/luli/project/mt_luxembourgish\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "def get_latest_file(pattern):\n",
    "    files = [f for f in os.listdir() if f.startswith(pattern)]\n",
    "    return max(files, key=os.path.getmtime) if files else None\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "\n",
    "# Define a function to generate the translation prompt\n",
    "def generate_translation_prompt(text, language_1=\"Luxembourgish\", language_2=\"English\"):\n",
    "    prompt_template = \"\"\"Please translate the following {language_1} text into {language_2}. Please answer me with only translated text!\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    {Text}\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    translation_prompt = PromptTemplate(\n",
    "        input_variables=[\"language_1\", \"language_2\", text],\n",
    "        template=prompt_template\n",
    "    )\n",
    "    \n",
    "    return translation_prompt.format(language_1=language_1, language_2=language_2, Text=text)\n",
    "\n",
    "\n",
    "def initialize_pipeline(config):\n",
    "    model_path = config[\"model_name\"]\n",
    "    if not model_path:\n",
    "        raise ValueError(\"model_name is not set\")\n",
    "    load_in_4bit, load_in_8bit = config[\"current_load_in_4bit\"], config[\"current_load_in_4bit\"]\n",
    "\n",
    "    if config[\"if_loading_quantization\"]:\n",
    "        nf4_config = BitsAndBytesConfig(load_in_4bit=load_in_4bit, load_in_8bit=load_in_8bit, bnb_4bit_compute_dtype=torch.float16)\n",
    "    else:\n",
    "        nf4_config = None\n",
    "\n",
    "    # update configuration in model generation\n",
    "    config_updates = config[\"model_config\"]\n",
    "    generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "    for key, value in config_updates.items():\n",
    "        setattr(generation_config, key, value)\n",
    "    \n",
    "    text_pipeline = pipeline(\"text-generation\", model=model_path, torch_dtype=torch.float32, device_map=config[\"device\"])\n",
    "    # text_pipeline.model.generation_config = generation_config # This needs to write a blog on that\n",
    "    text_pipeline.generation_config = generation_config\n",
    "    return text_pipeline\n",
    "\n",
    "def generate_text(pipeline, prompt):\n",
    "    response = pipeline(prompt)[0][\"generated_text\"]\n",
    "    return response\n",
    "\n",
    "def find_most_recent_date(df, date_column):\n",
    "    \"\"\"Finds the most recent date in the specified date column of the DataFrame.\"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "    most_recent_date = df[date_column].max()\n",
    "    return most_recent_date\n",
    "\n",
    "def get_latest_file(prefix):\n",
    "    files = [f for f in os.listdir() if f.startswith(prefix) and f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        return None\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    return latest_file\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column=\"subsentence\"):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "def translate_batch_LLM(config, df):\n",
    "    translator = initialize_pipeline(config)\n",
    "    df[\"prompts_inputs\"] = df[config[\"text_column\"]].apply(generate_translation_prompt)\n",
    "\n",
    "    # Determine output file name \n",
    "    prefix = config[\"prefix\"]\n",
    "    latest_file = get_latest_file(prefix)\n",
    "    if latest_file and not bool(config.get(\"is_new_file\", False)):\n",
    "        output_file = latest_file\n",
    "    else:\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"{prefix}_{current_time}.csv\"\n",
    "\n",
    "    # Load data and check starting index\n",
    "    texts = df[config[\"text_column\"]].tolist()\n",
    "    start_idx = (\n",
    "        load_checkpoint(latest_file, df, config[\"text_column\"]) \n",
    "        if (latest_file and not bool(config.get(\"is_new_file\", False))) \n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    print(\"Start From Index: \", start_idx)\n",
    "    texts = df[\"prompts_inputs\"].to_list()\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    for i in tqdm(range(start_idx, len(texts), batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        translated_batch = translator(batch, pad_token_id=translator.tokenizer.eos_token_id, return_full_text=False)\n",
    "\n",
    "        for j, text in enumerate(batch):\n",
    "            updated_row = df.iloc[i + j].copy()\n",
    "            updated_row[\"translated_text\"] = translated_batch[j][0]['generated_text']\n",
    "            updated_dataframe = pd.DataFrame([updated_row])\n",
    "            \n",
    "            if i == start_idx and j == 0 and config[\"is_new_file\"]:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"w\", header=True)\n",
    "            else:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"/mnt/lscratch/users/luli/model/Llama-3.2-3B-Instruct\",\n",
    "    \"if_loading_quantization\": False,\n",
    "    \"current_load_in_4bit\": True,\n",
    "    \"current_load_in_8bit\": False,\n",
    "    \"model_config\": {\n",
    "        \"temperature\": 0.1, # necessary\n",
    "        \"max_tokens\": 512, # necessary\n",
    "        \"top_p\": 0.9, # necessary\n",
    "        \"do_sample\": True, # necessary\n",
    "        \"max_new_tokens\": 512, # necessary\n",
    "        \"max_length\": 512, # necessary\n",
    "    },\n",
    "    \"batch_size\": 5, # use this to accelerate the translation process\n",
    "    \"prefix\": \"translation_LLM_huggingface_pipeline_\", # necessary\n",
    "    \"text_column\": \"subsentence\", # necessary\n",
    "    \"device\": \"auto\",\n",
    "    \"is_new_file\": False\n",
    "}\n",
    "\n",
    "dataset_df = pd.read_csv(\"NC_lux_subsentences_test.csv\")\n",
    "translate_batch_LLM(config = config, df = dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM running with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "project_root = \"/home/users/luli/project/mt_luxembourgish\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Define a function to generate the translation prompt\n",
    "def generate_translation_prompt(text, language_1 = \"Luxembourgish\", language_2 = \"English\"):\n",
    "    prompt_template = \"\"\"Please translate the following {language_1} text into {language_2}. Please answer me with only translated text!\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    {Text}\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    translation_prompt = PromptTemplate(\n",
    "        input_variables=[\"language_1\", \"language_2\", text],\n",
    "        template=prompt_template\n",
    "    )\n",
    "    \n",
    "    return translation_prompt.format(language_1=language_1, language_2=language_2, Text=text)\n",
    "\n",
    "\n",
    "def generate_text_with_ollama(config, prompt):\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": config[\"model_name\"],\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": \"json\", # Without this, the api calling will be blocked\n",
    "        \"options\": config[\"options\"],\n",
    "    }\n",
    "    response = requests.post(config[\"server_url\"], headers=config[\"headers\"], data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = json.loads(response.text)\n",
    "        actual_response = data[\"response\"]\n",
    "        return actual_response\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "def get_latest_file(prefix):\n",
    "    files = [f for f in os.listdir() if f.startswith(prefix) and f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        return None\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    return latest_file\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "def translate_batch_ollama(config, df):\n",
    "    df[\"prompts_inputs\"] = df[config[\"text_column\"]].apply(generate_translation_prompt)\n",
    "    \n",
    "    # Determine output file name \n",
    "    prefix = config[\"prefix\"]\n",
    "    latest_file = get_latest_file(prefix)\n",
    "    if latest_file and not bool(config.get(\"is_new_file\", False)):\n",
    "        output_file = latest_file\n",
    "    else:\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"{prefix}_{current_time}.csv\"\n",
    "\n",
    "    # Load data and check starting index\n",
    "    texts = df[config[\"text_column\"]].tolist()\n",
    "    start_idx = (\n",
    "        load_checkpoint(latest_file, df, config[\"text_column\"]) \n",
    "        if (latest_file and not bool(config.get(\"is_new_file\", False))) \n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    print(\"Start From Index: \", start_idx)\n",
    "    texts = df[\"prompts_inputs\"].to_list()\n",
    "\n",
    "    for i in tqdm(range(start_idx, len(texts), config[\"batch_size\"]), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = texts[i:i + config[\"batch_size\"]]\n",
    "        translated_batch = [generate_text_with_ollama(config, text) for text in batch]\n",
    "\n",
    "        for j, text in enumerate(batch):\n",
    "            updated_row = df.iloc[i + j].copy()\n",
    "            updated_row[\"translated_text\"] = translated_batch[j]\n",
    "            updated_dataframe = pd.DataFrame([updated_row])\n",
    "            \n",
    "            if i == start_idx and j == 0 and not latest_file:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"w\", header=True)\n",
    "            else:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"llama3.1:8b\", # necessary\n",
    "    \"server_url\": \"http://localhost:11434/api/generate\", # necessary\n",
    "    \"headers\": {\"Content-Type\": \"application/json\"},  # necessary\n",
    "    # model setting options\n",
    "    \"options\": {\n",
    "        \"temperature\": 0.1, # necessary\n",
    "        \"max_tokens\": 512, # necessary\n",
    "        \"top_p\": 0.9, # necessary\n",
    "        \"do_sample\": True, # necessary\n",
    "        \"max_new_tokens\": 512, # necessary\n",
    "        \"max_length\": 512, # necessary\n",
    "        \"num_ctx\": 2048, # necessary for acclelerating the translation process\n",
    "    },\n",
    "    \"batch_size\": 1, # use this to accelerate the translation process\n",
    "    \"prefix\": \"translation_LLM_ollama\", # necessary\n",
    "    \"text_column\": \"subsentence\", # necessary\n",
    "    \"is_new_file\": True\n",
    "}\n",
    "\n",
    "input_file = \"NC_lux_subsentences_test.csv\"\n",
    "dataset_df = pd.read_csv(input_file)\n",
    "translate_batch_ollama(config=config, df=dataset_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM running with vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start From Index:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|          | 0/30 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=5260): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fed362b7e90>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/urllib3/connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/urllib3/connection.py:441\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/http/client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1277\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1037\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m \n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/http/client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/urllib3/connection.py:279\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/urllib3/connection.py:214\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7fed362b7e90>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5260): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fed362b7e90>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 128\u001b[0m\n\u001b[1;32m    126\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNC_lux_subsentences_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m dataset_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(input_file)\n\u001b[0;32m--> 128\u001b[0m \u001b[43mtranslate_batch_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 97\u001b[0m, in \u001b[0;36mtranslate_batch_vllm\u001b[0;34m(config, df)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_idx, \u001b[38;5;28mlen\u001b[39m(texts), config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslating\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     96\u001b[0m     batch \u001b[38;5;241m=\u001b[39m texts[i:i \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m---> 97\u001b[0m     translated_batch \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mgenerate_text_with_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[1;32m    100\u001b[0m         updated_row \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m+\u001b[39m j]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[1], line 97\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_idx, \u001b[38;5;28mlen\u001b[39m(texts), config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslating\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     96\u001b[0m     batch \u001b[38;5;241m=\u001b[39m texts[i:i \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m---> 97\u001b[0m     translated_batch \u001b[38;5;241m=\u001b[39m [\u001b[43mgenerate_text_with_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[1;32m    100\u001b[0m         updated_row \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m+\u001b[39m j]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m, in \u001b[0;36mgenerate_text_with_vllm\u001b[0;34m(config, prompt)\u001b[0m\n\u001b[1;32m     39\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m     42\u001b[0m }\n\u001b[1;32m     43\u001b[0m payload\u001b[38;5;241m.\u001b[39mupdate(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 45\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserver_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     48\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.conda/envs/mt_lux_env/lib/python3.11/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5260): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fed362b7e90>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "project_root = \"/home/users/luli/project/mt_luxembourgish\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Define a function to generate the translation prompt\n",
    "def generate_translation_prompt(text, language_1=\"Luxembourgish\", language_2=\"English\"):\n",
    "    prompt_template = \"\"\"Please translate the following {language_1} text into {language_2}. Please answer me with only translated text!\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    {Text}\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    translation_prompt = PromptTemplate(\n",
    "        input_variables=[\"language_1\", \"language_2\", \"Text\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "    \n",
    "    return translation_prompt.format(language_1=language_1, language_2=language_2, Text=text)\n",
    "\n",
    "def generate_text_with_vllm(config, prompt):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": config[\"model_name\"],\n",
    "        \"prompt\": prompt,\n",
    "    }\n",
    "    payload.update(config[\"options\"])\n",
    "\n",
    "    response = requests.post(config[\"server_url\"], headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get(\"generated_text\", \"\")\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "\n",
    "def get_latest_file(prefix):\n",
    "    files = [f for f in os.listdir() if f.startswith(prefix) and f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        return None\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "\n",
    "def translate_batch_vllm(config, df):\n",
    "    df[\"prompts_inputs\"] = df[config[\"text_column\"]].apply(generate_translation_prompt)\n",
    "    \n",
    "    # Determine output file name \n",
    "    prefix = config[\"prefix\"]\n",
    "    latest_file = get_latest_file(prefix)\n",
    "    if latest_file and not bool(config.get(\"is_new_file\", False)):\n",
    "        output_file = latest_file\n",
    "    else:\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"{prefix}_{current_time}.csv\"\n",
    "\n",
    "    # Load data and check starting index\n",
    "    texts = df[config[\"text_column\"]].tolist()\n",
    "    start_idx = (\n",
    "        load_checkpoint(latest_file, df, config[\"text_column\"]) \n",
    "        if (latest_file and not bool(config.get(\"is_new_file\", False))) \n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    print(\"Start From Index: \", start_idx)\n",
    "    texts = df[\"prompts_inputs\"].to_list()\n",
    "\n",
    "    for i in tqdm(range(start_idx, len(texts), config[\"batch_size\"]), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = texts[i:i + config[\"batch_size\"]]\n",
    "        translated_batch = [generate_text_with_vllm(config, text) for text in batch]\n",
    "\n",
    "        for j, text in enumerate(batch):\n",
    "            updated_row = df.iloc[i + j].copy()\n",
    "            updated_row[\"translated_text\"] = translated_batch[j]\n",
    "            updated_dataframe = pd.DataFrame([updated_row])\n",
    "            \n",
    "            if i == start_idx and j == 0 and not latest_file:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"w\", header=True)\n",
    "            else:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=2 python -m vllm.entrypoints.openai.api_server --model /mnt/lscratch/users/luli/model/Llama-3.2-3B-Instruct --tensor-parallel-size 4 --port 5260 --device cuda --dtype float16\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"/mnt/lscratch/users/luli/model/Llama-3.2-3B-Instruct\",  # Change to your model's name in vllm\n",
    "    \"server_url\": \"http://localhost:5260/v1/completions\",  # vllm server URL\n",
    "    \"options\": {\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    \"batch_size\": 1,\n",
    "    \"prefix\": \"translation_LLM_vllm\",\n",
    "    \"text_column\": \"subsentence\",\n",
    "    \"is_new_file\": True\n",
    "}\n",
    "\n",
    "input_file = \"NC_lux_subsentences_test.csv\"\n",
    "dataset_df = pd.read_csv(input_file)\n",
    "translate_batch_vllm(config=config, df=dataset_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_lux_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
