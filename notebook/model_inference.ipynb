{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Inference Using the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/causalAnalysis/lib/python3.9/site-packages/IPython/utils/_process_posix.py:148\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    149\u001b[0m flush \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush\n",
      "File \u001b[0;32m/opt/anaconda3/envs/causalAnalysis/lib/python3.9/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_poll \u001b[38;5;241m=\u001b[39m use_poll\n",
      "File \u001b[0;32m/opt/anaconda3/envs/causalAnalysis/lib/python3.9/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc\u001b[38;5;241m.\u001b[39mpid\n",
      "File \u001b[0;32m/opt/anaconda3/envs/causalAnalysis/lib/python3.9/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPtyProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/causalAnalysis/lib/python3.9/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_read)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip install fasttext langid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapt install g++\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install spacy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/causalAnalysis/lib/python3.9/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/causalAnalysis/lib/python3.9/site-packages/IPython/utils/_process_posix.py:164\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    159\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241m.\u001b[39msendline(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "349.69s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext langid\n",
    "!apt install g++\n",
    "!pip install spacy\n",
    "!python -m spacy download xx_ent_wiki_sm\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "!pip install ctranslate2\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLLB Running Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def get_latest_file(pattern):\n",
    "    files = [f for f in os.listdir() if f.startswith(pattern)]\n",
    "    return max(files, key=os.path.getmtime) if files else None\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "def translate_batch(config, df):\n",
    "    # Initialize model and tokenizer\n",
    "    model_path = config[\"model_name\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    \n",
    "    translator = pipeline(\n",
    "        \"translation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        src_lang=config[\"src_lang\"],\n",
    "        tgt_lang=config[\"tgt_lang\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        device=config[\"device\"]\n",
    "    )\n",
    "\n",
    "    # Load data and check starting index\n",
    "    texts = df[config[\"text_column\"]].tolist()\n",
    "    start_idx = load_checkpoint(latest_file, df, config[\"text_column\"]) if latest_file else 0\n",
    "    print(\"Start From Index: \", start_idx)\n",
    "    texts = df[config[\"text_column\"]].to_list()\n",
    "    \n",
    "    # Determine output file name \n",
    "    prefix = config[\"prefix\"]\n",
    "    latest_file = get_latest_file(prefix)\n",
    "    if latest_file:\n",
    "        output_file = latest_file\n",
    "    else:\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"{prefix}_{current_time}.csv\"\n",
    "\n",
    "    # Batch translation and saving\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    for i in tqdm(range(start_idx, len(texts), batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        translated_batch = translator(batch)\n",
    "        \n",
    "        for j, text in enumerate(batch):\n",
    "            updated_row = df.iloc[i + j].copy()\n",
    "            updated_row[\"translated_text\"] = translated_batch[j][\"translation_text\"]\n",
    "            updated_dataframe = pd.DataFrame([updated_row])\n",
    "            \n",
    "            mode = \"w\" if i == start_idx and j == 0 and start_idx == 0 else \"a\"\n",
    "            header = mode == \"w\"\n",
    "            updated_dataframe.to_csv(output_file, index=False, mode=mode, header=header)\n",
    "\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "\n",
    "# Unified configuration dictionary\n",
    "config = {\n",
    "    \"model_name\": \"/home/lujun_li/projects/base_models/nllb-200-3.3B\",\n",
    "    \"src_lang\": \"ltz_Latn\",\n",
    "    \"tgt_lang\": \"eng_Latn\",\n",
    "    \"device\": 'cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "    \"max_length\": 360,\n",
    "    \"batch_size\": 5,\n",
    "    \"text_column\": \"subsentence\",\n",
    "    \"prefix\": \"translation_nllb_\"\n",
    "}\n",
    "\n",
    "input_file = \"NC_lux_subsentences_test.csv\"\n",
    "dataset_df = pd.read_csv(input_file)\n",
    "translate_batch(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM running Local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a function to generate the translation prompt\n",
    "def generate_translation_prompt(text, language_1=\"English\", language_2=\"Luxembourgish\"):\n",
    "    prompt_template = \"\"\"Please translate the following {language_1} text into {language_2}. Please answer me with only translated text!\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    {Text}\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    translation_prompt = PromptTemplate(\n",
    "        input_variables=[\"language_1\", \"language_2\", text],\n",
    "        template=prompt_template\n",
    "    )\n",
    "    \n",
    "    return translation_prompt.format(language_1=language_1, language_2=language_2, Text=text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "def initialize_pipeline(model_config_dict):\n",
    "    model_path = model_config_dict[\"CURRENT_RUNNING_MODEL\"]\n",
    "    if not model_path:\n",
    "        raise ValueError(\"CURRENT_RUNNING_MODEL is not set\")\n",
    "    load_in_4bit, load_in_8bit = model_config_dict[\"CURRENT_LOAD_IN_4BIT\"], model_config_dict[\"CURRENT_LOAD_IN_8BIT\"]\n",
    "\n",
    "    if model_config_dict[\"IF_LOADING_QUANTIZATION\"]:\n",
    "        nf4_config = BitsAndBytesConfig(load_in_4bit=load_in_4bit, load_in_8bit=load_in_8bit, bnb_4bit_compute_dtype=torch.float16)\n",
    "    else:\n",
    "        nf4_config = None\n",
    "        \n",
    "    generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "    generation_config.do_sample = True\n",
    "    generation_config.max_length = 512\n",
    "    generation_config.pad_token_id = 0\n",
    "    generation_config.top_p = 0.9\n",
    "    generation_config.temperature = 0.1\n",
    "    generation_config.max_new_tokens = 512\n",
    "    \n",
    "    text_pipeline = pipeline(\"text-generation\", model=model_path, torch_dtype=torch.float32, device_map=\"auto\")\n",
    "    # text_pipeline.model.generation_config = generation_config # This needs to write a blog on that\n",
    "    text_pipeline.generation_config = generation_config\n",
    "    return text_pipeline\n",
    "\n",
    "def generate_text(pipeline, prompt):\n",
    "    response = pipeline(prompt)[0][\"generated_text\"]\n",
    "    return response\n",
    "\n",
    "def find_most_recent_date(df, date_column):\n",
    "    \"\"\"Finds the most recent date in the specified date column of the DataFrame.\"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "    most_recent_date = df[date_column].max()\n",
    "    return most_recent_date\n",
    "\n",
    "def get_latest_file(pattern=\"translation_*.csv\"):\n",
    "    files = [f for f in os.listdir() if f.startswith(\"translation_LLM_\") and f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        return None\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    return latest_file\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column=\"subsentence\"):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "def translate_batch_LLM(translator, df, text_column=\"subsentence\", batch_size=1):\n",
    "\n",
    "    df[\"prompts_inputs\"] = df[text_column].apply(generate_translation_prompt)\n",
    "    latest_file = get_latest_file()\n",
    "    # Determine output file name based on the latest file\n",
    "    if latest_file:\n",
    "        output_file = latest_file\n",
    "    else:\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"translation_LLM_{current_time}.csv\"\n",
    "\n",
    "    # Determine start index based on checkpoint file\n",
    "    start_idx = load_checkpoint(latest_file, df, text_column) if latest_file else 0\n",
    "    print (\"Start From Index: \", start_idx)\n",
    "    texts = df[\"prompts_inputs\"].to_list()\n",
    "\n",
    "    for i in tqdm(range(start_idx, len(texts), batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        translated_batch = translator(batch)\n",
    "\n",
    "        for j, text in enumerate(batch):\n",
    "            updated_row = df.iloc[i + j].copy()\n",
    "            updated_row[\"translated_text\"] = translated_batch[j][0]['generated_text']\n",
    "            updated_dataframe = pd.DataFrame([updated_row])\n",
    "            \n",
    "            if i == start_idx and j == 0 and not latest_file:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"w\", header=True)\n",
    "            else:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "model_config_dict = {\n",
    "    \"CURRENT_RUNNING_MODEL\": \"/home/lujun_li/projects/base_models/Llama-3.2-3B-Instruct\",\n",
    "    \"IF_LOADING_QUANTIZATION\": False,\n",
    "    \"CURRENT_LOAD_IN_4BIT\":    True,\n",
    "    \"CURRENT_LOAD_IN_8BIT\":    False\n",
    "}\n",
    "\n",
    "translator = initialize_pipeline(model_config_dict)\n",
    "dataset_df = pd.read_csv(\"/home/lujun_li/projects/mt_luxembourgish/data/NC_lux/NC_lux_subsentences_test.csv\")\n",
    "translate_batch_LLM(translator = translator, df = dataset_df, text_column=\"subsentence\", batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM running with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a function to generate the translation prompt\n",
    "def generate_translation_prompt(text, language_1 = \"English\", language_2 = \"Luxembourgish\"):\n",
    "    prompt_template = \"\"\"Please translate the following {language_1} text into {language_2}. Please answer me with only translated text!\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    {Text}\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    translation_prompt = PromptTemplate(\n",
    "        input_variables=[\"language_1\", \"language_2\", text],\n",
    "        template=prompt_template\n",
    "    )\n",
    "    \n",
    "    return translation_prompt.format(language_1=language_1, language_2=language_2, Text=text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "project_root = \"/Users/lujun.li/projects/mt_luxembourgish/\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "\n",
    "def generate_text_with_ollama(config, prompt):\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": config[\"model_name\"],\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": \"json\", # Without this, the api calling will be blocked\n",
    "        \"options\": config[\"options\"],\n",
    "    }\n",
    "    response = requests.post(config[\"server_url\"], headers=config[\"headers\"], data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = json.loads(response.text)\n",
    "        actual_response = data[\"response\"]\n",
    "        return actual_response\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "def get_latest_file(prefix):\n",
    "    files = [f for f in os.listdir() if f.startswith(prefix) and f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        return None\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    return latest_file\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "def translate_batch_ollama(config, df):\n",
    "    df[\"prompts_inputs\"] = df[config[\"text_column\"]].apply(generate_translation_prompt)\n",
    "    prefix = config[\"prefix\"]\n",
    "    latest_file = get_latest_file(prefix)\n",
    "    \n",
    "    if latest_file:\n",
    "        output_file = latest_file\n",
    "    else:\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"{prefix}_{current_time}.csv\"\n",
    "\n",
    "    start_idx = load_checkpoint(latest_file, df, config[\"text_column\"]) if latest_file else 0\n",
    "    print(\"Start From Index: \", start_idx)\n",
    "    texts = df[\"prompts_inputs\"].to_list()\n",
    "\n",
    "    for i in tqdm(range(start_idx, len(texts), config[\"batch_size\"]), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = texts[i:i + config[\"batch_size\"]]\n",
    "        translated_batch = [generate_text_with_ollama(config, text) for text in batch]\n",
    "\n",
    "        for j, text in enumerate(batch):\n",
    "            updated_row = df.iloc[i + j].copy()\n",
    "            updated_row[\"translated_text\"] = translated_batch[j]\n",
    "            updated_dataframe = pd.DataFrame([updated_row])\n",
    "            \n",
    "            if i == start_idx and j == 0 and not latest_file:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"w\", header=True)\n",
    "            else:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"llama3.1:8b\", # necessary\n",
    "    \"server_url\": \"http://localhost:11434/api/generate\", # necessary\n",
    "    \"headers\": {\"Content-Type\": \"application/json\"},  # necessary\n",
    "    # model setting options\n",
    "    \"options\": {\n",
    "        \"temperature\": 0.1, # necessary\n",
    "        \"max_tokens\": 512, # necessary\n",
    "        \"top_p\": 0.9, # necessary\n",
    "        \"do_sample\": True, # necessary\n",
    "        \"max_new_tokens\": 512, # necessary\n",
    "        \"max_length\": 512, # necessary\n",
    "        \"num_ctx\": 2048, # necessary for acclelerating the translation process\n",
    "    },\n",
    "    \"batch_size\": 1, # use this to accelerate the translation process\n",
    "    \"prefix\": \"translation_LLM_ollama\", # necessary\n",
    "    \"text_column\": \"subsentence\", # necessary\n",
    "}\n",
    "\n",
    "input_file = \"NC_lux_subsentences_test.csv\"\n",
    "dataset_df = pd.read_csv(input_file)\n",
    "translate_batch_ollama(config=config, df=dataset_df)\n",
    "\n",
    "# curl http://localhost:11434/api/generate -d '{\n",
    "#   \"model\": \"llama3.1:8b\",\n",
    "#   \"prompt\": \"Please translate the following Luxembourgish text into English. Please answer me with only translated text! Dat léich ënnert anerem dorun, datt wärend der Coronakris d'Téléconsultatiounen agefouert goufen, vun deene méi jonk Leit méi profitéiert hätten.2020 hu ronn 6.700 Patiente vun enger Consultatioun bei engem Psychiater profitéiert - alles an allem ass et eng Hausse vun 2,1 Prozent. Wärend d'Zuel vu Patienten iwwert d'Joer gekuckt 2019 relativ stabel war, goufen et 2020 staark Variatiounen. D'Zuel vu Leit, déi an d'Consultatioun komm sinn, ass am 2.\",\n",
    "#   \"format\": \"json\",\n",
    "#   \"stream\": false,\n",
    "# }'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM running with vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
