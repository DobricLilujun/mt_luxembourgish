{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Inference Using the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext langid\n",
    "!apt install g++\n",
    "!pip install spacy\n",
    "!python -m spacy download xx_ent_wiki_sm\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "!pip install ctranslate2\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLLB Running Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "def get_latest_file(pattern=\"translation_*.csv\"):\n",
    "    files = [f for f in os.listdir() if f.startswith(\"translation_\") and f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        return None\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    return latest_file\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column=\"subsentence\"):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "# Translate a batch of texts and save the results to a CSV file\n",
    "def translate_batch(translator, df, text_column=\"subsentence\", batch_size=1, is_checkpoint=False):\n",
    "    # Find the latest file by date\n",
    "    latest_file = get_latest_file()\n",
    "    start_idx = 0\n",
    "    # Determine output file name\n",
    "    if latest_file and is_checkpoint:\n",
    "        output_file = latest_file\n",
    "        # Determine start index based on checkpoint file\n",
    "        start_idx = load_checkpoint(latest_file, df, text_column) if latest_file else 0\n",
    "        print (\"Start From Index: \", start_idx)\n",
    "    else:\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"translation_{current_time}.csv\"\n",
    "    \n",
    "    texts = df[text_column].to_list()\n",
    "    for i in tqdm(range(start_idx, len(texts), batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        translated_batch = translator(batch)\n",
    "\n",
    "        for j, text in enumerate(batch):\n",
    "            updated_row = df.iloc[i + j].copy()\n",
    "            updated_row[\"translated_text\"] = translated_batch[j]['translation_text']  #\n",
    "            updated_dataframe = pd.DataFrame([updated_row])\n",
    "            \n",
    "            if i == start_idx and j == 0 and not latest_file:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"w\", header=True)\n",
    "            else:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "model_path = \"/home/lujun_li/projects/base_models/nllb-200-3.3B\"\n",
    "src_lang=\"ltz_Latn\"\n",
    "tgt_lang=\"eng_Latn\"\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "translator = pipeline(\n",
    "    'translation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    src_lang=src_lang,\n",
    "    tgt_lang=tgt_lang,\n",
    "    max_length=360,\n",
    "    device=device,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "dataset_df = pd.read_csv(\"/home/lujun_li/projects/mt_luxembourgish/data/NC_lux/NC_lux_subsentences_test.csv\")\n",
    "translate_batch(translator = translator, df = dataset_df, text_column=\"subsentence\", batch_size=5, is_checkpoint=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM running Local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a function to generate the translation prompt\n",
    "def generate_translation_prompt(text, language_1=\"Luxembourgish\", language_2=\"English\"):\n",
    "    prompt_template = \"\"\"Please translate the following {language_1} text into {language_2}. Please answer me with only translated text!\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    {Text}\n",
    "\n",
    "    ---------------------------------- Text to be translated ----------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    translation_prompt = PromptTemplate(\n",
    "        input_variables=[\"language_1\", \"language_2\", text],\n",
    "        template=prompt_template\n",
    "    )\n",
    "    \n",
    "    return translation_prompt.format(language_1=language_1, language_2=language_2, Text=text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "def initialize_pipeline(model_config_dict):\n",
    "    model_path = model_config_dict[\"CURRENT_RUNNING_MODEL\"]\n",
    "    if not model_path:\n",
    "        raise ValueError(\"CURRENT_RUNNING_MODEL is not set\")\n",
    "    load_in_4bit, load_in_8bit = model_config_dict[\"CURRENT_LOAD_IN_4BIT\"], model_config_dict[\"CURRENT_LOAD_IN_8BIT\"]\n",
    "\n",
    "    if model_config_dict[\"IF_LOADING_QUANTIZATION\"]:\n",
    "        nf4_config = BitsAndBytesConfig(load_in_4bit=load_in_4bit, load_in_8bit=load_in_8bit, bnb_4bit_compute_dtype=torch.float16)\n",
    "    else:\n",
    "        nf4_config = None\n",
    "        \n",
    "    generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "    generation_config.do_sample = True\n",
    "    generation_config.max_length = 512\n",
    "    generation_config.pad_token_id = 0\n",
    "    generation_config.top_p = 0.9\n",
    "    generation_config.temperature = 0.1\n",
    "    generation_config.max_new_tokens = 512\n",
    "    \n",
    "    text_pipeline = pipeline(\"text-generation\", model=model_path, torch_dtype=torch.float32, device_map=\"auto\")\n",
    "    # text_pipeline.model.generation_config = generation_config # This needs to write a blog on that\n",
    "    text_pipeline.generation_config = generation_config\n",
    "    return text_pipeline\n",
    "\n",
    "def generate_text(pipeline, prompt):\n",
    "    response = pipeline(prompt)[0][\"generated_text\"]\n",
    "    return response\n",
    "\n",
    "def find_most_recent_date(df, date_column):\n",
    "    \"\"\"Finds the most recent date in the specified date column of the DataFrame.\"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "    most_recent_date = df[date_column].max()\n",
    "    return most_recent_date\n",
    "\n",
    "def get_latest_file(pattern=\"translation_*.csv\"):\n",
    "    files = [f for f in os.listdir() if f.startswith(\"translation_LLM_\") and f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        return None\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    return latest_file\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column=\"subsentence\"):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "def translate_batch_LLM(translator, df, text_column=\"subsentence\", batch_size=1):\n",
    "\n",
    "    df[\"prompts_inputs\"] = df[text_column].apply(generate_translation_prompt)\n",
    "    latest_file = get_latest_file()\n",
    "    # Determine output file name based on the latest file\n",
    "    if latest_file:\n",
    "        output_file = latest_file\n",
    "    else:\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"translation_LLM_{current_time}.csv\"\n",
    "\n",
    "    # Determine start index based on checkpoint file\n",
    "    start_idx = load_checkpoint(latest_file, df, text_column) if latest_file else 0\n",
    "    print (\"Start From Index: \", start_idx)\n",
    "    texts = df[\"prompts_inputs\"].to_list()\n",
    "\n",
    "    for i in tqdm(range(start_idx, len(texts), batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        translated_batch = translator(batch)\n",
    "\n",
    "        for j, text in enumerate(batch):\n",
    "            updated_row = df.iloc[i + j].copy()\n",
    "            updated_row[\"translated_text\"] = translated_batch[j][0]['generated_text']\n",
    "            updated_dataframe = pd.DataFrame([updated_row])\n",
    "            \n",
    "            if i == start_idx and j == 0 and not latest_file:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"w\", header=True)\n",
    "            else:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "model_config_dict = {\n",
    "    \"CURRENT_RUNNING_MODEL\": \"/home/lujun_li/projects/base_models/Llama-3.2-3B-Instruct\",\n",
    "    \"IF_LOADING_QUANTIZATION\": False,\n",
    "    \"CURRENT_LOAD_IN_4BIT\":    True,\n",
    "    \"CURRENT_LOAD_IN_8BIT\":    False\n",
    "}\n",
    "\n",
    "translator = initialize_pipeline(model_config_dict)\n",
    "dataset_df = pd.read_csv(\"/home/lujun_li/projects/mt_luxembourgish/data/NC_lux/NC_lux_subsentences_test.csv\")\n",
    "translate_batch_LLM(translator = translator, df = dataset_df, text_column=\"subsentence\", batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM running with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def generate_text_with_ollama(model_name, prompt):\n",
    "    url = f\"http://localhost:11434/api/generate\"  # Default local Ollama server endpoint\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"system\": \"\",\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stop\": None,\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "def find_most_recent_date(df, date_column):\n",
    "    \"\"\"Finds the most recent date in the specified date column of the DataFrame.\"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "    most_recent_date = df[date_column].max()\n",
    "    return most_recent_date\n",
    "\n",
    "def get_latest_file(pattern=\"translation_*.csv\"):\n",
    "    files = [f for f in os.listdir() if f.startswith(\"translation_LLM_\") and f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        return None\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    return latest_file\n",
    "\n",
    "def load_checkpoint(latest_file, df, text_column=\"subsentence\"):\n",
    "    if latest_file:\n",
    "        translated_df = pd.read_csv(latest_file)\n",
    "        translated_texts = translated_df[text_column].tolist()\n",
    "        start_idx = len(translated_texts)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return start_idx\n",
    "\n",
    "def translate_batch_ollama(model_name, df, text_column=\"subsentence\", batch_size=1):\n",
    "    df[\"prompts_inputs\"] = df[text_column].apply(generate_translation_prompt)\n",
    "    latest_file = get_latest_file()\n",
    "    # Determine output file name based on the latest file\n",
    "    if latest_file:\n",
    "        output_file = latest_file\n",
    "    else:\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"translation_LLM_{current_time}.csv\"\n",
    "\n",
    "    # Determine start index based on checkpoint file\n",
    "    start_idx = load_checkpoint(latest_file, df, text_column) if latest_file else 0\n",
    "    print(\"Start From Index: \", start_idx)\n",
    "    texts = df[\"prompts_inputs\"].to_list()\n",
    "\n",
    "    for i in tqdm(range(start_idx, len(texts), batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        translated_batch = [generate_text_with_ollama(model_name, text) for text in batch]\n",
    "\n",
    "        for j, text in enumerate(batch):\n",
    "            updated_row = df.iloc[i + j].copy()\n",
    "            updated_row[\"translated_text\"] = translated_batch[j]\n",
    "            updated_dataframe = pd.DataFrame([updated_row])\n",
    "            \n",
    "            if i == start_idx and j == 0 and not latest_file:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"w\", header=True)\n",
    "            else:\n",
    "                updated_dataframe.to_csv(output_file, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "def generate_translation_prompt(text):\n",
    "    return f\"Translate the following text into Luxembourgish:\\n\\n{text}\"\n",
    "\n",
    "# Configuration for Ollama\n",
    "model_name = \"llama-3.2-3b-instruct\"\n",
    "\n",
    "# Load the dataset and start translation\n",
    "dataset_df = pd.read_csv(\"/home/lujun_li/projects/mt_luxembourgish/data/NC_lux/NC_lux_subsentences_test.csv\")\n",
    "translate_batch_ollama(model_name=model_name, df=dataset_df, text_column=\"subsentence\", batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM running with vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_luxembourgish_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
