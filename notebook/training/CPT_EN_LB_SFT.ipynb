{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Pre-training using LLM recipie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets transformers torch tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments passed:\n"
     ]
    }
   ],
   "source": [
    "# ====================================================IMPORT=================================================================\n",
    "\n",
    "from datasets import DatasetDict, load_dataset, load_from_disk, Dataset\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ.update(\n",
    "    {\n",
    "        # \"NCCL_P2P_DISABLE\": \"1\",\n",
    "        # \"NCCL_IB_DISABLE\": \"1\",\n",
    "        # \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "        # \"CUDA_VISIBLE_DEVICES\": \"3,2,1\",\n",
    "    }\n",
    ")\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import argparse\n",
    "\n",
    "\n",
    "# ========================== CMD Argument Parser ==========================\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=10, help=\"Batch size per device during training\")\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=10, help=\"Batch size per device during evaluation\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
    "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "#     parser.add_argument(\"--model_name\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
    "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "print(\"Arguments passed:\")\n",
    "# print(f\"Train Batch Size: {args.per_device_train_batch_size}\")\n",
    "# print(f\"Eval Batch Size: {args.per_device_eval_batch_size}\")\n",
    "# print(f\"Number of Epochs: {args.num_train_epochs}\")\n",
    "# print(f\"Learning Rate: {args.learning_rate}\")\n",
    "# print(f\"Project Root: {args.project_root}\")\n",
    "# print(f\"Training Dataset Path: {args.training_dataset_path}\")\n",
    "# print(f\"Model Name: {args.model_name}\")\n",
    "\n",
    "# learning_rate = args.learning_rate # Learning rate for the optimizer\n",
    "# per_device_train_batch_size = args.per_device_train_batch_size  # Batch size for training per device\n",
    "# per_device_eval_batch_size = args.per_device_eval_batch_size  # Batch size for evaluation per device\n",
    "# num_train_epochs = args.num_train_epochs  # Number of epochs for training\n",
    "# training_dataset_path = args.training_dataset_path\n",
    "# project_root = args.project_root\n",
    "# model_name = args.model_name\n",
    "# resume_from_checkpoint = args.resume_from_checkpoint\n",
    "# resume_checkpoint_path = args.resume_checkpoint_path\n",
    "\n",
    "learning_rate = 1e-6 # Learning rate for the optimizer\n",
    "per_device_train_batch_size = 1  # Batch size for training per device\n",
    "per_device_eval_batch_size = 1  # Batch size for evaluation per device\n",
    "num_train_epochs = 1  # Number of epochs for training\n",
    "training_dataset_path = \"data/training_dataset/dataset_GPT_split.jsonl\"\n",
    "project_root = \"/home/snt/projects_lujun/mt_luxembourgish\"\n",
    "model_name = \"/home/snt/llm_models/Llama-3.2-1B-Instruct\"\n",
    "resume_from_checkpoint = False\n",
    "resume_checkpoint_path = None\n",
    "device=\"cuda:0\"\n",
    "\n",
    "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
    "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n",
    "\n",
    "\n",
    "train_ratio = 0.001  # Number of samples to be used for training and evaluation\n",
    "learning_rate = 1e-6  # Learning rate for the optimizer\n",
    "per_device_train_batch_size = 1  # Batch size for training per device\n",
    "per_device_eval_batch_size = 1  # Batch size for evaluation per device\n",
    "num_train_epochs = 5  # Number of epochs for training\n",
    "weight_decay = 0.01  # Weight decay rate for regularization\n",
    "MAX_LEN = 512  # Maximum sequence length for model inputs\n",
    "warmup_ratio = 0.5\n",
    "\n",
    "logging_steps = 100\n",
    "evaluation_strategy=\"epoch\"\n",
    "save_strategy=\"epoch\"\n",
    "eval_steps=500\n",
    "\n",
    "max_grad_norm = 0.3\n",
    "fp16 = True\n",
    "resume_from_checkpoint = False\n",
    "\n",
    "project_root = \"/home/snt/projects_lujun/mt_luxembourgish\"\n",
    "model_name = (\n",
    "    # \"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\"\n",
    "    \"/home/snt/llm_models/Llama-3.2-1B-Instruct\"\n",
    ")\n",
    "val_dataset_path = os.path.abspath(os.path.join(project_root, \"data/fake_targets/flores_devtest_arrow\"))\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, \"data/training_dataset/dataset_GPT_split.jsonl\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 203/203 [00:00<00:00, 7921.73 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 590.66 examples/s]\n",
      "Map: 100%|██████████| 203/203 [00:00<00:00, 6981.91 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 534.37 examples/s]\n",
      "Map: 100%|██████████| 203/203 [00:00<00:00, 3545.38 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 490.45 examples/s]\n",
      "Map: 100%|██████████| 203/203 [00:00<00:00, 2101.94 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 283.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ========================== Main Training Code ==========================\n",
    "\n",
    "val_dataset_path = os.path.abspath(os.path.join(project_root, \"data/fake_targets/flores_devtest_arrow\"))\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Load dataset\n",
    "if train_dataset_path.endswith(\".jsonl\"):\n",
    "    dataset = Dataset.from_json(train_dataset_path)  # Ensure correct format\n",
    "else:\n",
    "    dataset = load_from_disk(train_dataset_path)\n",
    "\n",
    "# Filter by split\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "\n",
    "# Select subset\n",
    "train_dataset = train_dataset.select(range(int(len(train_dataset) * train_ratio)))\n",
    "val_dataset = val_dataset.select(range(int(len(val_dataset) * train_ratio)))  # Avoid out-of-range error\n",
    "\n",
    "# Rename columns\n",
    "train_dataset = train_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def create_prompt(\n",
    "    sample, mode=\"train\", src_lng=\"English\", tgt_lng=\"Luxembourgish\", tokenizer=None\n",
    "):\n",
    "    if tokenizer is None or tokenizer.eos_token is None:\n",
    "        raise ValueError(\"A tokenizer with a defined EOS token is required.\")\n",
    "\n",
    "    system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "    input_text = sample[src_lng.capitalize()].strip()  # Extract the input text.\n",
    "    response = (\n",
    "        sample[tgt_lng.capitalize()].strip() if tgt_lng.capitalize() in sample else \"\"\n",
    "    )  # Extract the target text.\n",
    "\n",
    "    # Get the EOS token from the tokenizer.\n",
    "    eos_token = tokenizer.eos_token\n",
    "\n",
    "    # Construct the full prompt.\n",
    "    full_prompt = (\n",
    "\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\" + system_message + \"<|eot_id|>\" +  # System Message\n",
    "\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + f\"Translate the English input text into Luxembourgish.\\n{input_text}. Do not include any additional information or unrelated content.\"  +  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # User Query\n",
    "    )\n",
    "\n",
    "    full_input_prompt = full_prompt\n",
    "    full_input_CPT_prompt = full_input_prompt + response + \"<|eot_id|>\"\n",
    "    labels = response + \"<|eot_id|>\"  # <|eot_id|> is end of generation\n",
    "    if mode == \"train\":\n",
    "        full_prompt += response + eos_token\n",
    "\n",
    "    return {\"full_input_prompt\": full_input_prompt, \"full_input_CPT_prompt\": full_input_CPT_prompt, \"labels\": labels}\n",
    "\n",
    "train_dataset_CPT = train_dataset.map(\n",
    "    lambda sample: {\n",
    "        \"full_input_CPT_prompt\": create_prompt(sample, mode=\"train\", tokenizer=tokenizer)[\"full_input_CPT_prompt\"]\n",
    "    }\n",
    ").select_columns([\"full_input_CPT_prompt\"])\n",
    "\n",
    "val_dataset_CPT = val_dataset.map(\n",
    "    lambda sample: {\n",
    "        \"full_input_CPT_prompt\": create_prompt(sample, mode=\"train\", tokenizer=tokenizer)[\"full_input_CPT_prompt\"]\n",
    "    }\n",
    ").select_columns([\"full_input_CPT_prompt\"])\n",
    "\n",
    "\n",
    "# train_dataset_SFT = train_dataset.map(\n",
    "#     lambda sample: {\n",
    "#         **create_prompt(sample, mode=\"train\", tokenizer=tokenizer),\n",
    "#         \"inputs\": create_prompt(sample, mode=\"train\", tokenizer=tokenizer)[\"full_input_prompt\"],\n",
    "#         \"outputs\": create_prompt(sample, mode=\"train\", tokenizer=tokenizer)[\"labels\"]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# val_dataset_SFT = val_dataset.map(\n",
    "#     lambda sample: {\n",
    "#         **create_prompt(sample, mode=\"train\", tokenizer=tokenizer),\n",
    "#         \"inputs\": create_prompt(sample, mode=\"train\", tokenizer=tokenizer)[\"full_input_prompt\"],\n",
    "#         \"outputs\": create_prompt(sample, mode=\"train\", tokenizer=tokenizer)[\"labels\"]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# dataset_CPT = DatasetDict({\"train\": train_dataset_CPT, \"val\": val_dataset_CPT})\n",
    "# dataset_SFT = DatasetDict({\"train\": train_dataset_SFT, \"val\": val_dataset_SFT})\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "def tokenize_function_CPT(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"full_input_CPT_prompt\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "# def tokenize_function_SFT(examples):\n",
    "#     tokenized_example = tokenizer(\n",
    "#         examples[\"inputs\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=MAX_LEN,\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "#     tokenized_example[\"labels\"] = tokenizer(examples[\"labels\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")[\"input_ids\"]\n",
    "#     return tokenized_example\n",
    "\n",
    "\n",
    "tokenized_train_dataset_CPT = train_dataset_CPT.map(\n",
    "    tokenize_function_CPT, batched=True, remove_columns=[\"full_input_CPT_prompt\"]\n",
    ")\n",
    "tokenized_val_dataset_CPT = val_dataset_CPT.map(\n",
    "    tokenize_function_CPT, batched=True, remove_columns=[\"full_input_CPT_prompt\"]\n",
    ")\n",
    "\n",
    "# tokenized_train_dataset_SFT = train_dataset_SFT.map(\n",
    "#     tokenize_function_SFT, batched=True, remove_columns=[\"inputs\",\"outputs\"]\n",
    "# )   \n",
    "\n",
    "# tokenized_val_dataset_SFT = val_dataset_SFT.map(\n",
    "#     tokenize_function_SFT, batched=True, remove_columns=[\"inputs\",\"outputs\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant for translation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Translate the English input text into Luxembourgish.\n",
      "In both the women's and men's events, the mass start was on the program as part of the Tour de Ski in Switzerland. The women were challenged on a 10-kilometer circuit, and it was Linn Svahn from Sweden who managed to secure victory in the final sprint. In second place was Russia's Julia Stupak, ahead of the American Jessie Diggins.. Do not include any additional information or unrelated content.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Bei den Dammen a bei den Häre stoung an der Schwäiz de Massestart am Kader vum Tour de Ski um Programm. D'Damme waren op engem Circuit iwwer 10 Kilometer gefuerdert a sou war et d'Linn Svahn aus Schweden, dat sech um Enn am Zilsprint d'Victoire séchere konnt. Op déi zweet Plaz ass d'Julia Stupak aus Russland komm, virun der US-Amerikanerin Jessie Diggins.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print (train_dataset_CPT[0][\"full_input_CPT_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1235814400 || all params: 1235814400 || trainable%: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='812' max='1015' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 812/1015 05:46 < 01:26, 2.34 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.807000</td>\n",
       "      <td>2.629273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.916800</td>\n",
       "      <td>2.416902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.282000</td>\n",
       "      <td>2.558568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.775800</td>\n",
       "      <td>2.668485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training CPT.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================TRAINING=================================================================\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "\n",
    "def train_ddp_accelerate_cpt():\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.config.use_cache = False\n",
    "    current = time.time()\n",
    "    if resume_from_checkpoint:\n",
    "        output_dir = resume_checkpoint_path\n",
    "    else:\n",
    "        input_file_name = training_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        output_dir = f\"logs/fit_{current}_{train_ratio}_{input_file_name}\"\n",
    "\n",
    "    print(print_trainable_parameters(model))\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        save_strategy=save_strategy,\n",
    "        # eval_steps=eval_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=fp16,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"tensorboard\",\n",
    "        ddp_find_unused_parameters=False,\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=False,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset_CPT,\n",
    "        eval_dataset=tokenized_val_dataset_CPT,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    print(\"Finished training CPT.\")\n",
    "\n",
    "# def train_ddp_accelerate_sft():\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#     model.config.use_cache = False\n",
    "#     current = time.time()\n",
    "#     if resume_from_checkpoint:\n",
    "#         output_dir = resume_checkpoint_path\n",
    "#     else:\n",
    "#         input_file_name = training_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "#         output_dir = f\"logs/fit_{current}_{train_ratio}_{input_file_name}\"\n",
    "\n",
    "#     print(print_trainable_parameters(model))\n",
    "    \n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         per_device_train_batch_size=per_device_train_batch_size,\n",
    "#         per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "#         warmup_ratio=warmup_ratio,\n",
    "#         evaluation_strategy=evaluation_strategy,\n",
    "#         save_strategy=save_strategy,\n",
    "#         # eval_steps=eval_steps,\n",
    "#         logging_steps=logging_steps,\n",
    "#         learning_rate=learning_rate,\n",
    "#         weight_decay=weight_decay,\n",
    "#         fp16=fp16,\n",
    "#         max_grad_norm=max_grad_norm,\n",
    "#         group_by_length=True,\n",
    "#         lr_scheduler_type=\"cosine\",\n",
    "#         report_to=\"tensorboard\",\n",
    "#         ddp_find_unused_parameters=False,\n",
    "#         remove_unused_columns=False,\n",
    "#         disable_tqdm=False,\n",
    "#         load_best_model_at_end=True,\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=tokenized_train_dataset_SFT,\n",
    "#         eval_dataset=tokenized_val_dataset_SFT,\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "#     )\n",
    "\n",
    "#     trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "#     print(\"Finished training CPT.\")\n",
    "    \n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "def main():\n",
    "    accelerator = Accelerator()\n",
    "    train_ddp_accelerate_cpt()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output:\n",
      " D'Konzept vun dem Fraktgesaga ass net ëmdee konservative. Et ass eng d’Verwaltungsschad vun der Staats- a Regierung afunge. Zu këtten ass den europäesche Unioun, déi an de Memberstaaten heescht, fir deemno sinn d'Aachtergëtt, d'Wirtschaftspolitik, die Monetarischoft, d'Zivilsicherheet a d'Internationaledes Kämpfungsregeln a war um Fall. Déi Unioun baut no deemno mam Memberstaaten Déi an deemno sinn d’Aachtergëtt fir den Interessen vu.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model_checkpoint = \"/home/snt/projects_lujun/mt_luxembourgish/logs/fit_1740413339.3789797_0.001_dataset_GPT_split/checkpoint-812\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "input_text = \"\"\"The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.. Do not include any additional information or unrelated content.\"\"\"\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "# Construct the full prompt.\n",
    "full_prompt = (\n",
    "\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\" + system_message + \"<|eot_id|>\" +  # System Message\n",
    "\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + f\"Translate the English input text into Luxembourgish.\\n{input_text}. Do not include any additional information or unrelated content.\"  +  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # User Query\n",
    "    )\n",
    "\n",
    "# full_prompt = f\"Translate the English input text into Luxembourgish.\\n{input_text}. Do not include any additional information or unrelated content.\"\n",
    "\n",
    "# Tokenize input text\n",
    "encoded_input = tokenizer(full_prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "model_inputs = encoded_input.to(device)\n",
    "generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512 * 2,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "decoded_output = tokenizer.batch_decode(generated_ids)[0].replace(full_prompt, \"\")\n",
    "\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "for token in special_tokens:\n",
    "    decoded_output = decoded_output.replace(token, \"\")\n",
    "    \n",
    "print(\"Test Output:\\n\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_lux_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
