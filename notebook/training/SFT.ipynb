{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Pre-training using LLM recipie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: tensorboard in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: packaging in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (5.29.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers torch tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================IMPORT=================================================================\n",
    "\n",
    "from datasets import DatasetDict, load_dataset, load_from_disk, Dataset\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import argparse\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "# ========================== CMD Argument Parser ==========================\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=10, help=\"Batch size per device during training\")\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=10, help=\"Batch size per device during evaluation\")\n",
    "#     parser.add_argument(\"--src_lng\", type=str, default=\"English\", help=\"Source language default English\")\n",
    "#     parser.add_argument(\"--tgt_lng\", type=str, default=\"Luxembourgish\", help=\"Target language default Luxembourgish\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
    "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "#     parser.add_argument(\"--model_name\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
    "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "# print(\"Arguments passed:\")\n",
    "# print(f\"Train Batch Size: {args.per_device_train_batch_size}\")\n",
    "# print(f\"Eval Batch Size: {args.per_device_eval_batch_size}\")\n",
    "# print(f\"Number of Epochs: {args.num_train_epochs}\")\n",
    "# print(f\"Learning Rate: {args.learning_rate}\")\n",
    "# print(f\"Project Root: {args.project_root}\")\n",
    "# print(f\"Training Dataset Path: {args.training_dataset_path}\")\n",
    "# print(f\"Model Name: {args.model_name}\")\n",
    "# print(f\"tgt_lng: {args.tgt_lng}\")\n",
    "# print(f\"src_lng: {args.src_lng}\")\n",
    "# print(f\"Resume from checkpoint: {args.resume_from_checkpoint}\")\n",
    "# print(f\"Resume checkpoint path: {args.resume_checkpoint_path}\")\n",
    "\n",
    "# learning_rate = args.learning_rate # Learning rate for the optimizer\n",
    "# per_device_train_batch_size = args.per_device_train_batch_size  # Batch size for training per device\n",
    "# per_device_eval_batch_size = args.per_device_eval_batch_size  # Batch size for evaluation per device\n",
    "# num_train_epochs = args.num_train_epochs  # Number of epochs for training\n",
    "# training_dataset_path = args.training_dataset_path\n",
    "# project_root = args.project_root\n",
    "# model_name = args.model_name\n",
    "# resume_from_checkpoint = args.resume_from_checkpoint\n",
    "# resume_checkpoint_path = args.resume_checkpoint_path\n",
    "# src_lng = args.src_lng\n",
    "# tgt_lng = args.tgt_lng\n",
    "\n",
    "\n",
    "learning_rate = 1e-6 # Learning rate for the optimizer\n",
    "per_device_train_batch_size = 1  # Batch size for training per device\n",
    "per_device_eval_batch_size = 1  # Batch size for evaluation per device\n",
    "num_train_epochs = 3  # Number of epochs for training\n",
    "training_dataset_path = \"data/training_dataset/dataset_GPT_split.jsonl\"\n",
    "project_root = \"/home/snt/projects_lujun/mt_luxembourgish\"\n",
    "model_name = \"/home/snt/llm_models/Llama-3.2-1B-Instruct\"\n",
    "src_lng = \"English\"\n",
    "tgt_lng = \"Luxembourgish\"\n",
    "resume_from_checkpoint = False\n",
    "resume_checkpoint_path = None\n",
    "\n",
    "train_ratio = 0.001  # Number of samples to be used for training and evaluation\n",
    "device=\"cuda:0\"\n",
    "warmup_ratio = 0.5\n",
    "logging_steps = 100\n",
    "evaluation_strategy=\"epoch\"\n",
    "save_strategy=\"epoch\"\n",
    "eval_steps=500\n",
    "max_grad_norm = 0.3\n",
    "fp16 = True\n",
    "MAX_LEN = 512\n",
    "weight_decay = 0.01\n",
    "\n",
    "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
    "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n",
    "\n",
    "val_dataset_path = os.path.abspath(os.path.join(project_root, \"data/fake_targets/flores_devtest_arrow\"))\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, \"data/training_dataset/dataset_GPT_split.jsonl\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== Main Training Code ==========================\n",
    "\n",
    "val_dataset_path = os.path.abspath(os.path.join(project_root, \"data/fake_targets/flores_devtest_arrow\"))\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Load dataset\n",
    "if train_dataset_path.endswith(\".jsonl\"):\n",
    "    dataset = Dataset.from_json(train_dataset_path)  # Ensure correct format\n",
    "else:\n",
    "    dataset = load_from_disk(train_dataset_path)\n",
    "\n",
    "# Filter by split\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "\n",
    "# Select subset\n",
    "train_dataset = train_dataset.select(range(int(len(train_dataset) * train_ratio)))\n",
    "val_dataset = val_dataset.select(range(int(len(val_dataset) * train_ratio)))  # Avoid out-of-range error\n",
    "\n",
    "# Rename columns\n",
    "train_dataset = train_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def create_prompt(\n",
    "    sample, src_lng, tgt_lng, mode=\"train\", tokenizer=None\n",
    "):\n",
    "    if tokenizer is None or tokenizer.eos_token is None:\n",
    "        raise ValueError(\"A tokenizer with a defined EOS token is required.\")\n",
    "\n",
    "    system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "    input_text = sample[src_lng.capitalize()].strip()  # Extract the input text.\n",
    "    response = ( sample[tgt_lng.capitalize()].strip() if tgt_lng.capitalize() in sample else \"\")  # Extract the target text.\n",
    "\n",
    "    # Get the EOS token from the tokenizer.\n",
    "    eos_token = tokenizer.eos_token\n",
    "\n",
    "    # Construct the full prompt.\n",
    "    full_prompt = (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\" + system_message + \"<|eot_id|>\" +  # System Message\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + f\"Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\\n\\n{input_text}\"  +  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # User Query\n",
    "    )\n",
    "\n",
    "    if mode == \"train\":\n",
    "        full_prompt += (\"\\n\\n\" + response + eos_token)\n",
    "\n",
    "    return {\"full_prompt\": full_prompt}\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda sample: {\n",
    "        \"full_prompt\": create_prompt(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "    }\n",
    ").select_columns([\"full_prompt\"])\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda sample: {\n",
    "        \"full_prompt\": create_prompt(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "    }\n",
    ").select_columns([\"full_prompt\"])\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"full_prompt\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"full_prompt\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"full_prompt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Prompt:    **************************************************\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant for translation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\n",
      "\n",
      "In both the women's and men's events, the mass start was on the program as part of the Tour de Ski in Switzerland. The women were challenged on a 10-kilometer circuit, and it was Linn Svahn from Sweden who managed to secure victory in the final sprint. In second place was Russia's Julia Stupak, ahead of the American Jessie Diggins.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bei den Dammen a bei den Häre stoung an der Schwäiz de Massestart am Kader vum Tour de Ski um Programm. D'Damme waren op engem Circuit iwwer 10 Kilometer gefuerdert a sou war et d'Linn Svahn aus Schweden, dat sech um Enn am Zilsprint d'Victoire séchere konnt. Op déi zweet Plaz ass d'Julia Stupak aus Russland komm, virun der US-Amerikanerin Jessie Diggins.<|eot_id|>\n",
      "Example Prompt:    **************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Example Prompt:    \"+\"*\"*50)\n",
    "print (train_dataset[\"full_prompt\"][0])\n",
    "print(\"Example Prompt:    \"+\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# ====================================================TRAINING=================================================================\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "\n",
    "def train_ddp_accelerate_sft():\n",
    "    accelerator = Accelerator()\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.config.use_cache = False\n",
    "    model = accelerator.prepare(model)\n",
    "    current = time.time()\n",
    "    formatted_time = time.strftime(\"%m_%d_%H_%M\", time.localtime(current))\n",
    "    if resume_from_checkpoint:\n",
    "        output_dir = resume_checkpoint_path\n",
    "    else:\n",
    "        input_file_name = training_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        output_dir = f\"logs/{src_lng[:2]}_{tgt_lng[:2]}/fit_{formatted_time}_{train_ratio}_{input_file_name}\"\n",
    "\n",
    "    print(print_trainable_parameters(model))\n",
    "    \n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        save_strategy=save_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=fp16,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"tensorboard\",\n",
    "        ddp_find_unused_parameters=False,\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=False,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    print(\"Finished training SFT.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1235814400 || all params: 1235814400 || trainable%: 100.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='609' max='609' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [609/609 04:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.531100</td>\n",
       "      <td>2.496189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.680400</td>\n",
       "      <td>2.428447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.156300</td>\n",
       "      <td>2.555688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training SFT.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_ddp_accelerate_sft()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output:\n",
      " \n",
      "\n",
      "De Konzept vum éissten Zweeten Kandidaten ass net ohne Kämpft op de Leschter: E Pro Argument géif d’Wahl mä méi démokrateesch déi Weidergehalten. An deem Zuel géif den Éischte Exekutiv-Residenten an den Regierungsresidenten op d’Spill gréisst. Esou geet einfach, déi EU géif déi fir deenen am Vertrag mat de Membrancer ënnerkleeften ëm Beilechen. Zum Beispill fëren d'Mënschent, d'Traderpolt, d'Monetairpolt, d'Konkurrenzvirschrëfte an d'Zerwéierung vun der EU.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model_checkpoint = \"/home/snt/projects_lujun/mt_luxembourgish/logs/En_Lu/fit_02_26_02_15_0.001_dataset_GPT_split/checkpoint-609\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "input_text = \"\"\"The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.\"\"\"\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "# Construct the full prompt.\n",
    "full_prompt = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\" + system_message + \"<|eot_id|>\" +  # System Message\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + f\"Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\\n\\n{input_text}\"  +  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # User Query\n",
    ")\n",
    "\n",
    "# full_prompt = f\"Translate the English input text into Luxembourgish.\\n{input_text}. Do not include any additional information or unrelated content.\"\n",
    "\n",
    "# Tokenize input text\n",
    "encoded_input = tokenizer(full_prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "model_inputs = encoded_input.to(device)\n",
    "generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "decoded_output = tokenizer.batch_decode(generated_ids)[0].replace(full_prompt, \"\")\n",
    "\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "for token in special_tokens:\n",
    "    decoded_output = decoded_output.replace(token, \"\")\n",
    "    \n",
    "print(\"Test Output:\\n\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant for translation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\n",
      "\n",
      "The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print (full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_lux_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
