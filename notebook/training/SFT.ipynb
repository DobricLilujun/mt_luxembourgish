{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Pre-training using LLM recipie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (2.6.0)\n",
      "Requirement already satisfied: tensorboard in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: packaging in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Using cached triton-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.21.0)\n",
      "Using cached triton-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "Installing collected packages: triton\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.1.0\n",
      "    Uninstalling triton-3.1.0:\n",
      "      Successfully uninstalled triton-3.1.0\n",
      "Successfully installed triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers torch tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================IMPORT=================================================================\n",
    "\n",
    "from datasets import DatasetDict, load_dataset, load_from_disk, Dataset\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import argparse\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# ========================== CMD Argument Parser ==========================\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=10, help=\"Batch size per device during training\")\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=10, help=\"Batch size per device during evaluation\")\n",
    "#     parser.add_argument(\"--src_lng\", type=str, default=\"English\", help=\"Source language default English\")\n",
    "#     parser.add_argument(\"--tgt_lng\", type=str, default=\"Luxembourgish\", help=\"Target language default Luxembourgish\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
    "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "#     parser.add_argument(\"--model_name\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
    "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "# print(\"Arguments passed:\")\n",
    "# print(f\"Train Batch Size: {args.per_device_train_batch_size}\")\n",
    "# print(f\"Eval Batch Size: {args.per_device_eval_batch_size}\")\n",
    "# print(f\"Number of Epochs: {args.num_train_epochs}\")\n",
    "# print(f\"Learning Rate: {args.learning_rate}\")\n",
    "# print(f\"Project Root: {args.project_root}\")\n",
    "# print(f\"Training Dataset Path: {args.training_dataset_path}\")\n",
    "# print(f\"Model Name: {args.model_name}\")\n",
    "# print(f\"tgt_lng: {args.tgt_lng}\")\n",
    "# print(f\"src_lng: {args.src_lng}\")\n",
    "# print(f\"Resume from checkpoint: {args.resume_from_checkpoint}\")\n",
    "# print(f\"Resume checkpoint path: {args.resume_checkpoint_path}\")\n",
    "\n",
    "# learning_rate = args.learning_rate # Learning rate for the optimizer\n",
    "# per_device_train_batch_size = args.per_device_train_batch_size  # Batch size for training per device\n",
    "# per_device_eval_batch_size = args.per_device_eval_batch_size  # Batch size for evaluation per device\n",
    "# num_train_epochs = args.num_train_epochs  # Number of epochs for training\n",
    "# training_dataset_path = args.training_dataset_path\n",
    "# project_root = args.project_root\n",
    "# model_name = args.model_name\n",
    "# resume_from_checkpoint = args.resume_from_checkpoint\n",
    "# resume_checkpoint_path = args.resume_checkpoint_path\n",
    "# src_lng = args.src_lng\n",
    "# tgt_lng = args.tgt_lng\n",
    "\n",
    "\n",
    "learning_rate = 1e-6 # Learning rate for the optimizer\n",
    "per_device_train_batch_size = 1  # Batch size for training per device\n",
    "per_device_eval_batch_size = 1  # Batch size for evaluation per device\n",
    "num_train_epochs = 3  # Number of epochs for training\n",
    "training_dataset_path = \"data/training_dataset/dataset_GPT_split.jsonl\"\n",
    "project_root = \"/home/snt/projects_lujun/mt_luxembourgish\"\n",
    "model_name = \"/home/snt/llm_models/Llama-3.2-1B-Instruct\"\n",
    "src_lng = \"English\"\n",
    "tgt_lng = \"Luxembourgish\"\n",
    "resume_from_checkpoint = False\n",
    "resume_checkpoint_path = None\n",
    "\n",
    "train_ratio = 0.01  # Number of samples to be used for training and evaluation\n",
    "warmup_ratio = 0.5\n",
    "logging_steps = 300\n",
    "evaluation_strategy=\"steps\"\n",
    "save_strategy=\"epoch\"\n",
    "eval_steps=300\n",
    "max_grad_norm = 0.3\n",
    "fp16 = True\n",
    "MAX_LEN = 512\n",
    "weight_decay = 0.01\n",
    "\n",
    "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
    "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n",
    "\n",
    "val_dataset_path = os.path.abspath(os.path.join(project_root, \"data/fake_targets/flores_devtest_arrow\"))\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, \"data/training_dataset/dataset_GPT_split.jsonl\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2033/2033 [00:00<00:00, 4242.57 examples/s]\n",
      "Map: 100%|██████████| 36/36 [00:00<00:00, 3166.11 examples/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========================== Main Training Code ==========================\n",
    "\n",
    "\n",
    "val_dataset_path = os.path.abspath(os.path.join(project_root, \"data/fake_targets/flores_devtest_arrow\"))\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Load dataset\n",
    "if train_dataset_path.endswith(\".jsonl\"):\n",
    "    dataset = Dataset.from_json(train_dataset_path)  # Ensure correct format\n",
    "else:\n",
    "    dataset = load_from_disk(train_dataset_path)\n",
    "\n",
    "# Filter by split\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "\n",
    "# Select subset\n",
    "train_dataset = train_dataset.select(range(int(len(train_dataset) * train_ratio)))\n",
    "val_dataset = val_dataset.select(range(int(len(val_dataset) * train_ratio)))  # Avoid out-of-range error\n",
    "\n",
    "# Rename columns\n",
    "train_dataset = train_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# def create_prompt(\n",
    "#     sample, src_lng, tgt_lng, mode=\"train\", tokenizer=None\n",
    "# ):\n",
    "    # if tokenizer is None or tokenizer.eos_token is None:\n",
    "    #     raise ValueError(\"A tokenizer with a defined EOS token is required.\")\n",
    "\n",
    "    # system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "    # input_text = sample[src_lng.capitalize()].strip()  # Extract the input text.\n",
    "    # response = ( sample[tgt_lng.capitalize()].strip() if tgt_lng.capitalize() in sample else \"\")  # Extract the target text.\n",
    "\n",
    "    # # Get the EOS token from the tokenizer.\n",
    "    # eos_token = tokenizer.eos_token\n",
    "\n",
    "#     # Construct the full prompt.\n",
    "#     full_prompt = (\n",
    "#         \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\" + system_message + \"<|eot_id|>\" +  # System Message\n",
    "        # \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + f\"Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\\n\\n{input_text}\"  +  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # User Query\n",
    "    # )\n",
    "\n",
    "#     if mode == \"train\":\n",
    "#         full_prompt += (\"\\n\\n\" + response + eos_token)\n",
    "\n",
    "#     return {\"full_prompt\": full_prompt}\n",
    "\n",
    "def create_prompt(sample, src_lng, tgt_lng, mode=\"train\", tokenizer=None):\n",
    "\n",
    "    if tokenizer is None or tokenizer.eos_token is None:\n",
    "        raise ValueError(\"A tokenizer with a defined EOS token is required.\")\n",
    "\n",
    "    system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "    input_text = sample[src_lng.capitalize()].strip()  # Extract the input text.\n",
    "    response = ( sample[tgt_lng.capitalize()].strip() if tgt_lng.capitalize() in sample else \"\")  # Extract the target text.\n",
    "    question = f\"Translate the following English input text into Luxembourgish. Do not include any additional information or unrelated content.\\n\\n{input_text}\"\n",
    "    # Get the EOS token from the tokenizer.\n",
    "    eos_token = tokenizer.eos_token\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return { \"full_prompt\": full_prompt }\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda sample: {\n",
    "        \"full_prompt\": create_prompt(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "    }\n",
    ").select_columns([\"full_prompt\"])\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda sample: {\n",
    "        \"full_prompt\": create_prompt(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "    }\n",
    ").select_columns([\"full_prompt\"])\n",
    "\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer, mlm=False, return_tensors=\"pt\"\n",
    "# )\n",
    "\n",
    "def compute_lengths(dataset, tokenizer):\n",
    "    lengths = [len(tokenizer(sample[\"full_prompt\"])[\"input_ids\"]) for sample in dataset]\n",
    "    return lengths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_lengths = compute_lengths(train_dataset, tokenizer)\n",
    "val_lengths = compute_lengths(val_dataset, tokenizer)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_lengths, bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel(\"Tokenized Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Tokenized Train Lengths\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2033/2033 [00:00<00:00, 3279.50 examples/s]\n",
      "Map: 100%|██████████| 36/36 [00:00<00:00, 2130.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"full_prompt\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"full_prompt\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"full_prompt\"])\n",
    "\n",
    "\n",
    "instruction_template = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "response_template = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    instruction_template=instruction_template,\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Prompt:    **************************************************\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 28 Feb 2025\n",
      "\n",
      "You are a helpful AI assistant for translation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Translate the following English input text into Luxembourgish. Do not include any additional information or unrelated content.\n",
      "\n",
      "In both the women's and men's events, the mass start was on the program as part of the Tour de Ski in Switzerland. The women were challenged on a 10-kilometer circuit, and it was Linn Svahn from Sweden who managed to secure victory in the final sprint. In second place was Russia's Julia Stupak, ahead of the American Jessie Diggins.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bei den Dammen a bei den Häre stoung an der Schwäiz de Massestart am Kader vum Tour de Ski um Programm. D'Damme waren op engem Circuit iwwer 10 Kilometer gefuerdert a sou war et d'Linn Svahn aus Schweden, dat sech um Enn am Zilsprint d'Victoire séchere konnt. Op déi zweet Plaz ass d'Julia Stupak aus Russland komm, virun der US-Amerikanerin Jessie Diggins.<|eot_id|>\n",
      "Example Prompt:    **************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Example Prompt:    \"+\"*\"*50)\n",
    "print (train_dataset[\"full_prompt\"][0])\n",
    "print(\"Example Prompt:    \"+\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# ====================================================TRAINING=================================================================\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "\n",
    "def train_ddp_accelerate_sft():\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.config.use_cache = False\n",
    "    model = accelerator.prepare(model)\n",
    "    current = time.time()\n",
    "    formatted_time = time.strftime(\"%m_%d_%H_%M\", time.localtime(current))\n",
    "    if resume_from_checkpoint:\n",
    "        output_dir = resume_checkpoint_path\n",
    "    else:\n",
    "        input_file_name = training_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        output_dir = f\"logs/{src_lng[:2]}_{tgt_lng[:2]}/fit_{formatted_time}_{train_ratio}_{input_file_name}\"\n",
    "\n",
    "    print(print_trainable_parameters(model))\n",
    "    \n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        save_strategy=save_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=fp16,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"tensorboard\",\n",
    "        ddp_find_unused_parameters=False,\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=False,\n",
    "        # load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    print(\"Finished training SFT.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1235814400 || all params: 1235814400 || trainable%: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='6099' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 630/6099 03:03 < 26:35, 3.43 it/s, Epoch 0.31/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.817900</td>\n",
       "      <td>3.554043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.079700</td>\n",
       "      <td>2.979929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     train_ddp_accelerate_sft()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain_ddp_accelerate_sft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 63\u001b[0m, in \u001b[0;36mtrain_ddp_accelerate_sft\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m training_args \u001b[38;5;241m=\u001b[39m SFTConfig(\n\u001b[1;32m     31\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[1;32m     32\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39mnum_train_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# load_best_model_at_end=True,\u001b[39;00m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     54\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     55\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\u001b[39;00m\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training SFT.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/transformers/trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/accelerate/accelerator.py:2244\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2244\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_ddp_accelerate_sft()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output:\n",
      " \n",
      "\n",
      "De Konzept vum éissten Zweeten Kandidaten ass net ohne Kämpft op de Leschter: E Pro Argument géif d’Wahl mä méi démokrateesch déi Weidergehalten. An deem Zuel géif den Éischte Exekutiv-Residenten an den Regierungsresidenten op d’Spill gréisst. Esou geet einfach, déi EU géif déi fir deenen am Vertrag mat de Membrancer ënnerkleeften ëm Beilechen. Zum Beispill fëren d'Mënschent, d'Traderpolt, d'Monetairpolt, d'Konkurrenzvirschrëfte an d'Zerwéierung vun der EU.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model_checkpoint = \"/home/snt/projects_lujun/mt_luxembourgish/logs/En_Lu/fit_02_26_02_15_0.001_dataset_GPT_split/checkpoint-609\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "input_text = \"\"\"The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.\"\"\"\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "# Construct the full prompt.\n",
    "full_prompt = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\" + system_message + \"<|eot_id|>\" +  # System Message\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + f\"Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\\n\\n{input_text}\"  +  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # User Query\n",
    ")\n",
    "\n",
    "# full_prompt = f\"Translate the English input text into Luxembourgish.\\n{input_text}. Do not include any additional information or unrelated content.\"\n",
    "\n",
    "# Tokenize input text\n",
    "encoded_input = tokenizer(full_prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "model_inputs = encoded_input.to(device)\n",
    "generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "decoded_output = tokenizer.batch_decode(generated_ids)[0].replace(full_prompt, \"\")\n",
    "\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "for token in special_tokens:\n",
    "    decoded_output = decoded_output.replace(token, \"\")\n",
    "    \n",
    "print(\"Test Output:\\n\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant for translation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\n",
      "\n",
      "The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print (full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_lux_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
