{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Pre-training using LLM recipie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: tensorboard in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: packaging in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (5.29.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers torch tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ====================================================IMPORT=================================================================\n",
    "\n",
    "from datasets import DatasetDict, load_dataset, load_from_disk, Dataset\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import argparse\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "# ========================== CMD Argument Parser ==========================\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=10, help=\"Batch size per device during training\")\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=10, help=\"Batch size per device during evaluation\")\n",
    "#     parser.add_argument(\"--src_lng\", type=str, default=\"English\", help=\"Source language default English\")\n",
    "#     parser.add_argument(\"--tgt_lng\", type=str, default=\"Luxembourgish\", help=\"Target language default Luxembourgish\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
    "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "#     parser.add_argument(\"--model_name\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
    "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "# print(\"Arguments passed:\")\n",
    "# print(f\"Train Batch Size: {args.per_device_train_batch_size}\")\n",
    "# print(f\"Eval Batch Size: {args.per_device_eval_batch_size}\")\n",
    "# print(f\"Number of Epochs: {args.num_train_epochs}\")\n",
    "# print(f\"Learning Rate: {args.learning_rate}\")\n",
    "# print(f\"Project Root: {args.project_root}\")\n",
    "# print(f\"Training Dataset Path: {args.training_dataset_path}\")\n",
    "# print(f\"Model Name: {args.model_name}\")\n",
    "# print(f\"tgt_lng: {args.tgt_lng}\")\n",
    "# print(f\"src_lng: {args.src_lng}\")\n",
    "# print(f\"Resume from checkpoint: {args.resume_from_checkpoint}\")\n",
    "# print(f\"Resume checkpoint path: {args.resume_checkpoint_path}\")\n",
    "\n",
    "# learning_rate = args.learning_rate # Learning rate for the optimizer\n",
    "# per_device_train_batch_size = args.per_device_train_batch_size  # Batch size for training per device\n",
    "# per_device_eval_batch_size = args.per_device_eval_batch_size  # Batch size for evaluation per device\n",
    "# num_train_epochs = args.num_train_epochs  # Number of epochs for training\n",
    "# training_dataset_path = args.training_dataset_path\n",
    "# project_root = args.project_root\n",
    "# model_name = args.model_name\n",
    "# resume_from_checkpoint = args.resume_from_checkpoint\n",
    "# resume_checkpoint_path = args.resume_checkpoint_path\n",
    "# src_lng = args.src_lng\n",
    "# tgt_lng = args.tgt_lng\n",
    "\n",
    "\n",
    "learning_rate = 1e-6 # Learning rate for the optimizer\n",
    "per_device_train_batch_size = 1  # Batch size for training per device\n",
    "per_device_eval_batch_size = 1  # Batch size for evaluation per device\n",
    "num_train_epochs = 3  # Number of epochs for training\n",
    "training_dataset_path = \"data/training_dataset/dataset_GPT_split.jsonl\"\n",
    "project_root = \"/home/snt/projects_lujun/mt_luxembourgish\"\n",
    "model_name = \"/home/snt/llm_models/Llama-3.2-1B-Instruct\"\n",
    "src_lng = \"English\"\n",
    "tgt_lng = \"Luxembourgish\"\n",
    "resume_from_checkpoint = False\n",
    "resume_checkpoint_path = None\n",
    "\n",
    "train_ratio = 0.001  # Number of samples to be used for training and evaluation\n",
    "warmup_ratio = 0.5\n",
    "logging_steps = 300\n",
    "evaluation_strategy=\"steps\"\n",
    "save_strategy=\"epoch\"\n",
    "eval_steps=300\n",
    "max_grad_norm = 0.3\n",
    "fp16 = True\n",
    "MAX_LEN = 512\n",
    "weight_decay = 0.01\n",
    "\n",
    "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
    "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n",
    "\n",
    "val_dataset_path = os.path.abspath(os.path.join(project_root, \"data/fake_targets/flores_devtest_arrow\"))\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, \"data/training_dataset/dataset_GPT_split.jsonl\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== Main Training Code ==========================\n",
    "\n",
    "val_dataset_path = os.path.abspath(os.path.join(project_root, \"data/fake_targets/flores_devtest_arrow\"))\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Load dataset\n",
    "if train_dataset_path.endswith(\".jsonl\"):\n",
    "    dataset = Dataset.from_json(train_dataset_path)  # Ensure correct format\n",
    "else:\n",
    "    dataset = load_from_disk(train_dataset_path)\n",
    "\n",
    "# Filter by split\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "\n",
    "# Select subset\n",
    "train_dataset = train_dataset.select(range(int(len(train_dataset) * train_ratio)))\n",
    "val_dataset = val_dataset.select(range(int(len(val_dataset) * train_ratio)))  # Avoid out-of-range error\n",
    "\n",
    "# Rename columns\n",
    "train_dataset = train_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def create_prompt(\n",
    "    sample, src_lng, tgt_lng, mode=\"train\", tokenizer=None\n",
    "):\n",
    "    if tokenizer is None or tokenizer.eos_token is None:\n",
    "        raise ValueError(\"A tokenizer with a defined EOS token is required.\")\n",
    "\n",
    "    system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "    input_text = sample[src_lng.capitalize()].strip()  # Extract the input text.\n",
    "    response = ( sample[tgt_lng.capitalize()].strip() if tgt_lng.capitalize() in sample else \"\")  # Extract the target text.\n",
    "\n",
    "    # Get the EOS token from the tokenizer.\n",
    "    eos_token = tokenizer.eos_token\n",
    "\n",
    "    # Construct the full prompt.\n",
    "    full_prompt = (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\" + system_message + \"<|eot_id|>\" +  # System Message\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + f\"Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\\n\\n{input_text}\"  +  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # User Query\n",
    "    )\n",
    "\n",
    "    if mode == \"train\":\n",
    "        full_prompt += (\"\\n\\n\" + response + eos_token)\n",
    "\n",
    "    return {\"full_prompt\": full_prompt}\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda sample: {\n",
    "        \"full_prompt\": create_prompt(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "    }\n",
    ").select_columns([\"full_prompt\"])\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda sample: {\n",
    "        \"full_prompt\": create_prompt(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "    }\n",
    ").select_columns([\"full_prompt\"])\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"full_prompt\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"full_prompt\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"full_prompt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Prompt:    **************************************************\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant for translation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\n",
      "\n",
      "In both the women's and men's events, the mass start was on the program as part of the Tour de Ski in Switzerland. The women were challenged on a 10-kilometer circuit, and it was Linn Svahn from Sweden who managed to secure victory in the final sprint. In second place was Russia's Julia Stupak, ahead of the American Jessie Diggins.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bei den Dammen a bei den Häre stoung an der Schwäiz de Massestart am Kader vum Tour de Ski um Programm. D'Damme waren op engem Circuit iwwer 10 Kilometer gefuerdert a sou war et d'Linn Svahn aus Schweden, dat sech um Enn am Zilsprint d'Victoire séchere konnt. Op déi zweet Plaz ass d'Julia Stupak aus Russland komm, virun der US-Amerikanerin Jessie Diggins.<|eot_id|>\n",
      "Example Prompt:    **************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Example Prompt:    \"+\"*\"*50)\n",
    "print (train_dataset[\"full_prompt\"][0])\n",
    "print(\"Example Prompt:    \"+\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# ====================================================TRAINING=================================================================\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "\n",
    "def train_ddp_accelerate_sft():\n",
    "    accelerator = Accelerator()\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.config.use_cache = False\n",
    "    model = accelerator.prepare(model)\n",
    "    current = time.time()\n",
    "    formatted_time = time.strftime(\"%m_%d_%H_%M\", time.localtime(current))\n",
    "    if resume_from_checkpoint:\n",
    "        output_dir = resume_checkpoint_path\n",
    "    else:\n",
    "        input_file_name = training_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        output_dir = f\"logs/{src_lng[:2]}_{tgt_lng[:2]}/fit_{formatted_time}_{train_ratio}_{input_file_name}\"\n",
    "\n",
    "    print(print_trainable_parameters(model))\n",
    "    \n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        save_strategy=save_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=fp16,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"tensorboard\",\n",
    "        ddp_find_unused_parameters=False,\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=False,\n",
    "        # load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    print(\"Finished training SFT.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1235814400 || all params: 1235814400 || trainable%: 100.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='609' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 18/609 00:04 < 02:35, 3.81 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     train_ddp_accelerate_sft()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain_ddp_accelerate_sft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 60\u001b[0m, in \u001b[0;36mtrain_ddp_accelerate_sft\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m training_args \u001b[38;5;241m=\u001b[39m SFTConfig(\n\u001b[1;32m     30\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[1;32m     31\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39mnum_train_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     disable_tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     52\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     53\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training SFT.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/transformers/trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/accelerate/accelerator.py:2244\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2244\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mt_lux_env/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_ddp_accelerate_sft()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output:\n",
      " \n",
      "\n",
      "De Konzept vum éissten Zweeten Kandidaten ass net ohne Kämpft op de Leschter: E Pro Argument géif d’Wahl mä méi démokrateesch déi Weidergehalten. An deem Zuel géif den Éischte Exekutiv-Residenten an den Regierungsresidenten op d’Spill gréisst. Esou geet einfach, déi EU géif déi fir deenen am Vertrag mat de Membrancer ënnerkleeften ëm Beilechen. Zum Beispill fëren d'Mënschent, d'Traderpolt, d'Monetairpolt, d'Konkurrenzvirschrëfte an d'Zerwéierung vun der EU.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model_checkpoint = \"/home/snt/projects_lujun/mt_luxembourgish/logs/En_Lu/fit_02_26_02_15_0.001_dataset_GPT_split/checkpoint-609\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "input_text = \"\"\"The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.\"\"\"\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "# Construct the full prompt.\n",
    "full_prompt = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\" + system_message + \"<|eot_id|>\" +  # System Message\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + f\"Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\\n\\n{input_text}\"  +  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # User Query\n",
    ")\n",
    "\n",
    "# full_prompt = f\"Translate the English input text into Luxembourgish.\\n{input_text}. Do not include any additional information or unrelated content.\"\n",
    "\n",
    "# Tokenize input text\n",
    "encoded_input = tokenizer(full_prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "model_inputs = encoded_input.to(device)\n",
    "generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "decoded_output = tokenizer.batch_decode(generated_ids)[0].replace(full_prompt, \"\")\n",
    "\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "for token in special_tokens:\n",
    "    decoded_output = decoded_output.replace(token, \"\")\n",
    "    \n",
    "print(\"Test Output:\\n\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant for translation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\n",
      "\n",
      "The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print (full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_lux_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
