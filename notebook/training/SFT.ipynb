{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Pre-training using LLM recipie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (2025.2.15)\n",
      "Requirement already satisfied: datasets in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (3.3.2)\n",
      "Requirement already satisfied: transformers in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: tensorboard in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: matplotlib in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.2.7 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (2025.2.7)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.0.28.post3)\n",
      "Requirement already satisfied: bitsandbytes in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.45.3)\n",
      "Requirement already satisfied: triton>=3.0.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (3.2.0)\n",
      "Requirement already satisfied: packaging in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (24.2)\n",
      "Requirement already satisfied: tyro in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.9.16)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (7.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: numpy in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (2.2.3)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (1.4.0)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.15.2)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.14.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.29.1)\n",
      "Requirement already satisfied: hf_transfer in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.32.2)\n",
      "Requirement already satisfied: torchvision in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.21.0)\n",
      "Requirement already satisfied: filelock in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tensorboard) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: six>1.9 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: rich in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
      "Requirement already satisfied: cut_cross_entropy in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.7->unsloth) (25.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from diffusers->unsloth) (8.6.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tyro->unsloth) (4.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install unsloth datasets transformers torch tensorboard matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Parameter Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.11.10 (you have 3.11.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# ====================================================IMPORT=================================================================\n",
    "\n",
    "from datasets import DatasetDict, load_dataset, load_from_disk, Dataset\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import argparse\n",
    "\n",
    "from utils.utils_train import pre_process, create_prompt, print_trainable_parameters\n",
    "from unsloth import is_bfloat16_supported\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "\n",
    "# ========================== CMD Argument Parser ==========================\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=8, help=\"Batch size per device during training\")\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=8, help=\"Batch size per device during evaluation\")\n",
    "#     parser.add_argument(\"--src_lng\", type=str, default=\"English\", help=\"Source language default English\")\n",
    "#     parser.add_argument(\"--tgt_lng\", type=str, default=\"Luxembourgish\", help=\"Target language default Luxembourgish\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
    "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "#     parser.add_argument(\"--model_path\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
    "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
    "#     parser.add_argument(\"--r\", type=int, default=256, help=\"Number of random samples to be used for training\")\n",
    "#     parser.add_argument(\"--is_peft\", type=bool, default=False, help=\"Use PEFT\")\n",
    "#     parser.add_argument(\"--is_unsloth\", type=bool, default=False, help=\"Use UnSloth\")\n",
    "#     parser.add_argument(\"--is_train_response_only\", type=bool, default=True, help=\"Train response only\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "# print(\"Arguments passed:\")\n",
    "# print(f\"Train Batch Size: {args.per_device_train_batch_size}\")\n",
    "# print(f\"Eval Batch Size: {args.per_device_eval_batch_size}\")\n",
    "# print(f\"Number of Epochs: {args.num_train_epochs}\")\n",
    "# print(f\"Learning Rate: {args.learning_rate}\")\n",
    "# print(f\"Project Root: {args.project_root}\")\n",
    "# print(f\"Training Dataset Path: {args.training_dataset_path}\")\n",
    "# print(f\"Model path: {args.model_path}\")\n",
    "# print(f\"tgt_lng: {args.tgt_lng}\")\n",
    "# print(f\"src_lng: {args.src_lng}\")\n",
    "# print(f\"Resume from checkpoint: {args.resume_from_checkpoint}\")\n",
    "# print(f\"Resume checkpoint path: {args.resume_checkpoint_path}\")\n",
    "# print(f\"r: {args.r}\")\n",
    "# print(f\"is_peft: {args.is_peft}\")\n",
    "# print(f\"is_unsloth: {args.is_unsloth}\")\n",
    "# print(f\"is_train_response_only: {args.is_train_response_only}\")\n",
    "\n",
    "# learning_rate = args.learning_rate # Learning rate for the optimizer\n",
    "# per_device_train_batch_size = args.per_device_train_batch_size  # Batch size for training per device\n",
    "# per_device_eval_batch_size = args.per_device_eval_batch_size  # Batch size for evaluation per device\n",
    "# num_train_epochs = args.num_train_epochs  # Number of epochs for training\n",
    "# training_dataset_path = args.training_dataset_path\n",
    "# project_root = args.project_root\n",
    "# model_path = args.model_path\n",
    "# resume_from_checkpoint = args.resume_from_checkpoint\n",
    "# resume_checkpoint_path = args.resume_checkpoint_path\n",
    "# src_lng = args.src_lng\n",
    "# tgt_lng = args.tgt_lng\n",
    "# r = args.r\n",
    "# is_peft = args.is_peft\n",
    "# is_unsloth = args.is_unsloth\n",
    "# is_train_response_only = args.is_train_response_only\n",
    "\n",
    "## Params need to be set\n",
    "learning_rate = 1e-5 # Learning rate for the optimizer\n",
    "per_device_train_batch_size = 10  # Batch size for training per device\n",
    "per_device_eval_batch_size = 10  # Batch size for evaluation per device\n",
    "num_train_epochs = 5  # Number of epochs for training\n",
    "training_dataset_path = \"data/training_dataset/dataset_GPT_split.jsonl\"\n",
    "project_root = \"/home/snt/projects_lujun/mt_luxembourgish\"\n",
    "model_path = \"/home/snt/llm_models/Llama-3.2-1B-Instruct\"\n",
    "resume_from_checkpoint = False\n",
    "resume_checkpoint_path = None\n",
    "src_lng = \"English\"\n",
    "tgt_lng = \"Luxembourgish\"\n",
    "r = 32\n",
    "is_peft = False\n",
    "is_unsloth = False\n",
    "is_train_response_only = True\n",
    "\n",
    "\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "train_ratio = 0.005  # Number of samples to be used for training and evaluation\n",
    "warmup_ratio = 0.5\n",
    "logging_steps = 10\n",
    "evaluation_strategy=\"steps\"\n",
    "save_strategy=\"epoch\"\n",
    "eval_steps=10\n",
    "max_grad_norm = 0.3\n",
    "fp16 = not is_bfloat16_supported()\n",
    "MAX_LEN = 512\n",
    "weight_decay = 0.01\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "train_seed = 3407\n",
    "\n",
    "if is_peft:\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\",]\n",
    "    lora_alpha = 8\n",
    "    lora_dropout = 0\n",
    "    random_state = 3407\n",
    "\n",
    "current = time.time()\n",
    "formatted_time = time.strftime(\"%m_%d_%H_%M\", time.localtime(current))\n",
    "if resume_from_checkpoint:\n",
    "    output_dir = resume_checkpoint_path\n",
    "else:\n",
    "    if is_peft:\n",
    "        input_file_name = training_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        output_dir = f\"logs/peft_{r}_{src_lng[:2]}_{tgt_lng[:2]}/fit_{formatted_time}_{train_ratio}_{input_file_name}\"\n",
    "    else:\n",
    "        input_file_name = training_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        output_dir = f\"logs/full_{src_lng[:2]}_{tgt_lng[:2]}/fit_{formatted_time}_{train_ratio}_{input_file_name}\"\n",
    "\n",
    "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
    "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inputs before: 207011\n",
      "Length of inputs after: 204642\n",
      "Removed rows: 2369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204642/204642 [00:01<00:00, 127879.38 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204642/204642 [00:01<00:00, 131509.39 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1005/1005 [00:00<00:00, 2061.11 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 2417.78 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1005/1005 [00:00<00:00, 3906.97 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 1682.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ========================== dataset preparation ==========================\n",
    "\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "train_dataset_df = pd.read_json(train_dataset_path, lines=True)\n",
    "pre_processed_dataset_df = pre_process(train_dataset_df)\n",
    "\n",
    "if not isinstance(pre_processed_dataset_df, pd.DataFrame):\n",
    "    raise TypeError(\"data_preprocess should return a pandas DataFrame.\")\n",
    "\n",
    "dataset = Dataset.from_pandas(pre_processed_dataset_df)\n",
    "\n",
    "# Filter by split\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Select subset\n",
    "train_dataset = train_dataset.select(range(int(len(train_dataset) * train_ratio)))\n",
    "val_dataset = val_dataset.select(range(int(len(val_dataset) * train_ratio)))  # Avoid out-of-range error\n",
    "\n",
    "# Rename columns\n",
    "train_dataset = train_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda sample: {\n",
    "        \"full_prompt\": create_prompt(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "    }\n",
    ").select_columns([\"full_prompt\"])\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda sample: {\n",
    "        \"full_prompt\": create_prompt(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "    }\n",
    ").select_columns([\"full_prompt\"])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"full_prompt\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"full_prompt\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"full_prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models using unsloth or huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "trainable params: 1235814400 || all params: 1235814400 || trainable%: 100.0\n",
      "Layer: model.embed_tokens.weight, Shape: torch.Size([128256, 2048]), Trainable parameters: 262668288\n",
      "Layer: model.layers.0.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.0.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.0.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.0.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.0.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.0.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.0.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.0.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.0.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.1.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.1.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.1.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.1.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.1.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.1.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.1.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.1.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.1.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.2.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.2.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.2.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.2.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.2.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.2.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.2.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.2.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.2.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.3.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.3.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.3.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.3.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.3.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.3.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.3.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.3.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.3.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.4.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.4.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.4.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.4.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.4.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.4.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.4.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.4.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.4.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.5.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.5.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.5.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.5.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.5.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.5.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.5.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.5.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.5.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.6.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.6.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.6.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.6.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.6.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.6.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.6.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.6.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.6.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.7.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.7.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.7.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.7.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.7.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.7.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.7.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.7.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.7.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.8.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.8.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.8.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.8.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.8.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.8.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.8.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.8.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.8.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.9.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.9.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.9.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.9.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.9.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.9.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.9.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.9.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.9.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.10.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.10.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.10.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.10.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.10.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.10.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.10.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.10.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.10.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.11.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.11.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.11.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.11.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.11.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.11.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.11.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.11.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.11.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.12.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.12.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.12.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.12.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.12.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.12.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.12.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.12.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.12.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.13.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.13.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.13.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.13.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.13.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.13.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.13.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.13.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.13.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.14.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.14.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.14.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.14.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.14.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.14.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.14.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.14.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.14.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.15.self_attn.q_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.15.self_attn.k_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.15.self_attn.v_proj.weight, Shape: torch.Size([512, 2048]), Trainable parameters: 1048576\n",
      "Layer: model.layers.15.self_attn.o_proj.weight, Shape: torch.Size([2048, 2048]), Trainable parameters: 4194304\n",
      "Layer: model.layers.15.mlp.gate_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.15.mlp.up_proj.weight, Shape: torch.Size([8192, 2048]), Trainable parameters: 16777216\n",
      "Layer: model.layers.15.mlp.down_proj.weight, Shape: torch.Size([2048, 8192]), Trainable parameters: 16777216\n",
      "Layer: model.layers.15.input_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.layers.15.post_attention_layernorm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n",
      "Layer: model.norm.weight, Shape: torch.Size([2048]), Trainable parameters: 2048\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "\n",
    "# Using Unsloth Acceleration \n",
    "if is_unsloth:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = f\"unsloth/{model_name}\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "        # model_name = model_path,\n",
    "        max_seq_length = MAX_LEN,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    \n",
    "    if is_peft:\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r = r, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules = target_modules,\n",
    "            lora_alpha = lora_alpha,\n",
    "            lora_dropout = lora_dropout, # Supports any, but = 0 is optimized\n",
    "            bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "            random_state = random_state,\n",
    "            use_rslora = False,  # We support rank stabilized LoRA\n",
    "            loftq_config = None, # And without LoftQ\n",
    "        )\n",
    "\n",
    "# Using transformer huggingface   \n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    model.config.use_cache = False\n",
    "    if is_peft:\n",
    "        lora_config = LoraConfig(\n",
    "            r=r, \n",
    "            target_modules=target_modules, \n",
    "            lora_alpha=lora_alpha, \n",
    "            lora_dropout=lora_dropout, \n",
    "            bias=\"none\", \n",
    "            random_state=random_state,\n",
    "            use_rslora=False,\n",
    "            loftq_config=None  # And without LoftQ\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "print (model)\n",
    "print(print_trainable_parameters(model))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, Shape: {param.shape}, Trainable parameters: {param.numel()}\")\n",
    "    else:\n",
    "        print(f\"Layer: {name}, Shape: {param.shape}, Non-trainable parameters: {param.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging as transformers_logging\n",
    "from accelerate import Accelerator\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "def train_ddp_accelerate_sft():\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        save_strategy=save_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16= fp16,\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"tensorboard\",\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=False,\n",
    "        seed = train_seed,\n",
    "        ddp_find_unused_parameters=False, # Avoids warnings\n",
    "        dataloader_num_workers=4,  # Adjust number of workers based on hardware\n",
    "        # load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset = tokenized_train_dataset,\n",
    "        eval_dataset = tokenized_val_dataset,\n",
    "        dataset_text_field = \"full_prompt\",\n",
    "        max_seq_length = MAX_LEN,\n",
    "        data_collator = data_collator,\n",
    "        dataset_num_proc = 2,\n",
    "        packing = False, # Can make training 5x faster for short sequences.\n",
    "        args = training_args,\n",
    "    )\n",
    "\n",
    "    if is_train_response_only:\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "            response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        )\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    print(\"Finished training SFT.\")\n",
    "    return trainer_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX A6000. Max memory = 47.536 GB.\n",
      "0.0 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1005/1005 [00:00<00:00, 2317.94 examples/s]\n",
      "Applying chat template to train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1005/1005 [00:01<00:00, 759.40 examples/s] \n",
      "Truncating train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1005/1005 [00:00<00:00, 1540.97 examples/s]\n",
      "Converting eval dataset to ChatML (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 58.94 examples/s]\n",
      "Applying chat template to eval dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:01<00:00, 13.96 examples/s]\n",
      "Truncating eval dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 52.84 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1005/1005 [00:00<00:00, 2671.38 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 2256.35 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 07:25, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.676900</td>\n",
       "      <td>5.382022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.141100</td>\n",
       "      <td>1.283134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.030200</td>\n",
       "      <td>0.994903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.868839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.713400</td>\n",
       "      <td>0.774849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.597600</td>\n",
       "      <td>0.726333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.486400</td>\n",
       "      <td>0.696730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0.677242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.678876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.675392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>0.708359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>0.719258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>0.720880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>0.735199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.745416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.790262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.782160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.782983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.782169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.784072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.817010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.813150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.805314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.811596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.813403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training SFT.\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = None\n",
    "\n",
    "def main():\n",
    "    trainer_stats = train_ddp_accelerate_sft()\n",
    "    return trainer_stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer_stats = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447.7424 seconds used for training.\n",
      "7.46 minutes used for training.\n",
      "Peak reserved memory = 33.484 GB.\n",
      "Peak reserved memory for training = 33.484 GB.\n",
      "Peak reserved memory % of max memory = 70.439 %.\n",
      "Peak reserved memory for training % of max memory = 70.439 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "/home/snt/projects_lujun/mt_luxembourgish/logs/full_En_Lu/fit_03_01_22_42_0.005_dataset_GPT_split/checkpoint-102 does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "You are a helpful AI assistant for translation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\n",
      "\n",
      "The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here is the translation: The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules.\n",
      "End of translation.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "input_text = (\n",
    "    \"Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\\n\\n\"\n",
    "    \"The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. \"\n",
    "    \"On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, \"\n",
    "    \"for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.\"\n",
    ")\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/snt/projects_lujun/mt_luxembourgish/logs/full_En_Lu/fit_03_01_22_42_0.005_dataset_GPT_split/checkpoint-102\",\n",
    "    max_seq_length = MAX_LEN,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": input_text},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = MAX_LEN, use_cache = True,\n",
    "                         temperature = 1.0)\n",
    "print (tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output:\n",
      " \n",
      "\n",
      "De Konzept vum Ã©issten Zweeten Kandidaten ass net ohne KÃ¤mpft op de Leschter: E Pro Argument gÃ©if dâ€™Wahl mÃ¤ mÃ©i dÃ©mokrateesch dÃ©i Weidergehalten. An deem Zuel gÃ©if den Ã‰ischte Exekutiv-Residenten an den Regierungsresidenten op dâ€™Spill grÃ©isst. Esou geet einfach, dÃ©i EU gÃ©if dÃ©i fir deenen am Vertrag mat de Membrancer Ã«nnerkleeften Ã«m Beilechen. Zum Beispill fÃ«ren d'MÃ«nschent, d'Traderpolt, d'Monetairpolt, d'KonkurrenzvirschrÃ«fte an d'ZerwÃ©ierung vun der EU.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model_checkpoint = \"/home/snt/projects_lujun/mt_luxembourgish/logs/En_Lu/fit_02_26_02_15_0.001_dataset_GPT_split/checkpoint-609\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "system_message = f\"You are a helpful AI assistant for translation.\"\n",
    "input_text = \"\"\"The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.\"\"\"\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "# Construct the full prompt.\n",
    "full_prompt = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\" + system_message + \"<|eot_id|>\" +  # System Message\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + f\"Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\\n\\n{input_text}\"  +  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # User Query\n",
    ")\n",
    "\n",
    "# full_prompt = f\"Translate the English input text into Luxembourgish.\\n{input_text}. Do not include any additional information or unrelated content.\"\n",
    "\n",
    "# Tokenize input text\n",
    "encoded_input = tokenizer(full_prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "model_inputs = encoded_input.to(device)\n",
    "generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "decoded_output = tokenizer.batch_decode(generated_ids)[0].replace(full_prompt, \"\")\n",
    "\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "for token in special_tokens:\n",
    "    decoded_output = decoded_output.replace(token, \"\")\n",
    "    \n",
    "print(\"Test Output:\\n\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant for translation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Translate the English input text into Luxembourgish. Do not include any additional information or unrelated content.\n",
      "\n",
      "The concept of the lead candidate is not without controversy: One pro-argument is that the election becomes more democratic this way. On the other hand, the heads of state and government lose power as a result. Simply put, the EU is responsible for what is agreed upon between it and the member states, for example, for the internal market, trade policy, monetary policy, and competition rules, and there is also a common security and foreign policy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print (full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
