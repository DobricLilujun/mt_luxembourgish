{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inputs before: 207011\n",
      "Length of inputs after: 204642\n",
      "Removed rows: 2369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 204642/204642 [00:01<00:00, 122469.95 examples/s]\n",
      "Filter: 100%|██████████| 204642/204642 [00:01<00:00, 127054.31 examples/s]\n",
      "Map: 100%|██████████| 201/201 [00:00<00:00, 6363.06 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 621.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Translate the following English input text into Luxembourgish. Do not include any additional information or unrelated content.\n",
      "\n",
      "### Input:\n",
      "In both the women's and men's events, the mass start was on the program as part of the Tour de Ski in Switzerland. The women were challenged on a 10-kilometer circuit, and it was Linn Svahn from Sweden who managed to secure victory in the final sprint. In second place was Russia's Julia Stupak, ahead of the American Jessie Diggins.\n",
      "\n",
      "### Response:\n",
      "Bei den Dammen a bei den Häre stoung an der Schwäiz de Massestart am Kader vum Tour de Ski um Programm. D'Damme waren op engem Circuit iwwer 10 Kilometer gefuerdert a sou war et d'Linn Svahn aus Schweden, dat sech um Enn am Zilsprint d'Victoire séchere konnt. Op déi zweet Plaz ass d'Julia Stupak aus Russland komm, virun der US-Amerikanerin Jessie Diggins.<eos>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 201/201 [00:00<00:00, 1720.57 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 488.79 examples/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      ")\n",
      "trainable params: 2614341888 || all params: 2614341888 || trainable%: 100.0\n",
      "Layer: model.embed_tokens.weight, Shape: torch.Size([256000, 2304]), Trainable parameters: 589824000\n",
      "Layer: model.layers.0.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.0.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.0.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.0.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.0.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.0.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.0.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.0.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.0.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.0.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.0.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.1.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.1.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.1.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.1.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.1.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.1.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.1.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.1.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.1.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.1.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.1.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.2.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.2.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.2.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.2.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.2.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.2.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.2.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.2.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.2.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.2.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.2.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.3.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.3.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.3.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.3.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.3.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.3.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.3.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.3.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.3.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.3.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.3.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.4.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.4.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.4.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.4.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.4.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.4.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.4.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.4.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.4.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.4.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.4.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.5.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.5.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.5.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.5.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.5.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.5.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.5.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.5.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.5.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.5.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.5.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.6.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.6.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.6.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.6.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.6.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.6.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.6.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.6.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.6.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.6.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.6.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.7.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.7.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.7.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.7.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.7.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.7.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.7.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.7.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.7.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.7.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.7.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.8.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.8.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.8.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.8.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.8.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.8.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.8.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.8.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.8.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.8.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.8.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.9.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.9.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.9.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.9.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.9.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.9.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.9.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.9.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.9.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.9.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.9.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.10.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.10.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.10.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.10.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.10.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.10.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.10.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.10.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.10.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.10.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.10.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.11.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.11.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.11.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.11.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.11.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.11.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.11.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.11.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.11.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.11.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.11.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.12.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.12.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.12.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.12.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.12.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.12.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.12.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.12.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.12.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.12.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.12.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.13.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.13.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.13.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.13.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.13.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.13.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.13.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.13.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.13.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.13.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.13.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.14.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.14.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.14.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.14.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.14.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.14.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.14.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.14.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.14.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.14.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.14.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.15.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.15.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.15.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.15.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.15.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.15.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.15.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.15.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.15.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.15.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.15.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.16.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.16.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.16.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.16.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.16.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.16.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.16.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.16.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.16.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.16.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.16.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.17.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.17.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.17.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.17.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.17.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.17.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.17.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.17.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.17.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.17.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.17.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.18.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.18.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.18.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.18.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.18.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.18.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.18.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.18.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.18.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.18.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.18.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.19.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.19.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.19.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.19.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.19.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.19.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.19.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.19.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.19.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.19.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.19.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.20.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.20.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.20.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.20.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.20.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.20.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.20.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.20.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.20.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.20.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.20.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.21.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.21.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.21.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.21.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.21.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.21.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.21.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.21.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.21.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.21.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.21.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.22.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.22.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.22.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.22.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.22.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.22.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.22.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.22.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.22.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.22.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.22.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.23.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.23.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.23.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.23.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.23.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.23.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.23.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.23.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.23.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.23.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.23.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.24.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.24.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.24.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.24.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.24.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.24.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.24.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.24.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.24.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.24.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.24.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.25.self_attn.q_proj.weight, Shape: torch.Size([2048, 2304]), Trainable parameters: 4718592\n",
      "Layer: model.layers.25.self_attn.k_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.25.self_attn.v_proj.weight, Shape: torch.Size([1024, 2304]), Trainable parameters: 2359296\n",
      "Layer: model.layers.25.self_attn.o_proj.weight, Shape: torch.Size([2304, 2048]), Trainable parameters: 4718592\n",
      "Layer: model.layers.25.mlp.gate_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.25.mlp.up_proj.weight, Shape: torch.Size([9216, 2304]), Trainable parameters: 21233664\n",
      "Layer: model.layers.25.mlp.down_proj.weight, Shape: torch.Size([2304, 9216]), Trainable parameters: 21233664\n",
      "Layer: model.layers.25.input_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.25.post_attention_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.25.pre_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.layers.25.post_feedforward_layernorm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "Layer: model.norm.weight, Shape: torch.Size([2304]), Trainable parameters: 2304\n",
      "GPU = NVIDIA RTX A6000. Max memory = 47.536 GB.\n",
      "24.041 GB of memory reserved.\n",
      "Using GEMMA model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML (num_proc=2): 100%|██████████| 201/201 [00:00<00:00, 441.59 examples/s]\n",
      "Applying chat template to train dataset (num_proc=2): 100%|██████████| 201/201 [00:03<00:00, 64.46 examples/s]\n",
      "Truncating train dataset (num_proc=2): 100%|██████████| 201/201 [00:00<00:00, 366.53 examples/s]\n",
      "Converting eval dataset to ChatML (num_proc=2): 100%|██████████| 3/3 [00:00<00:00,  7.72 examples/s]\n",
      "Applying chat template to eval dataset (num_proc=2): 100%|██████████| 3/3 [00:02<00:00,  1.03 examples/s]\n",
      "Truncating eval dataset (num_proc=2): 100%|██████████| 3/3 [00:00<00:00,  7.15 examples/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 33.88 MiB is free. Including non-PyTorch memory, this process has 47.49 GiB memory in use. Of the allocated memory 46.43 GiB is allocated by PyTorch, and 766.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 390\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_stats\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 390\u001b[0m     trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# @title Show final memory and time stats\u001b[39;00m\n\u001b[1;32m    394\u001b[0m used_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 386\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 386\u001b[0m     trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ddp_accelerate_sft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_stats\n",
      "Cell \u001b[0;32mIn[2], line 370\u001b[0m, in \u001b[0;36mtrain_ddp_accelerate_sft\u001b[0;34m()\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_path \u001b[38;5;129;01mand\u001b[39;00m is_unsloth:\n\u001b[1;32m    363\u001b[0m         trainer \u001b[38;5;241m=\u001b[39m train_on_responses_only(\n\u001b[1;32m    364\u001b[0m             trainer,\n\u001b[1;32m    365\u001b[0m             instruction_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>user<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    366\u001b[0m             response_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>assistant<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    367\u001b[0m         )\n\u001b[0;32m--> 370\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training SFT.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer_stats\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py:2599\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2595\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped:\n\u001b[1;32m   2604\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/accelerate/optimizer.py:178\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/optim/adamw.py:232\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    229\u001b[0m     amsgrad: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     adamw(\n\u001b[1;32m    244\u001b[0m         params_with_grad,\n\u001b[1;32m    245\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    264\u001b[0m     )\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    161\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    163\u001b[0m         (),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m_get_scalar_dtype())\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    175\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    176\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    177\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 33.88 MiB is free. Including non-PyTorch memory, this process has 47.49 GiB memory in use. Of the allocated memory 46.43 GiB is allocated by PyTorch, and 766.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ====================================================IMPORT=================================================================\n",
    "\n",
    "from datasets import DatasetDict, load_dataset, load_from_disk, Dataset\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import argparse\n",
    "\n",
    "from utils.utils_train import pre_process, create_prompt, print_trainable_parameters, create_prompt_gemma\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from transformers import logging as transformers_logging\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "# ========================== CMD Argument Parser ==========================\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=8, help=\"Batch size per device during training\")\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=8, help=\"Batch size per device during evaluation\")\n",
    "#     parser.add_argument(\"--src_lng\", type=str, default=\"English\", help=\"Source language default English\")\n",
    "#     parser.add_argument(\"--tgt_lng\", type=str, default=\"Luxembourgish\", help=\"Target language default Luxembourgish\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
    "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "#     parser.add_argument(\"--model_path\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
    "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
    "#     parser.add_argument(\"--r\", type=int, default=256, help=\"Number of random samples to be used for training\")\n",
    "#     parser.add_argument(\"--is_peft\", type=bool, default=False, help=\"Use PEFT\")\n",
    "#     parser.add_argument(\"--is_unsloth\", type=bool, default=False, help=\"Use UnSloth\")\n",
    "#     parser.add_argument(\"--is_train_response_only\", type=bool, default=False, help=\"Train response only\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "# print(\"Arguments passed:\")\n",
    "# print(f\"Train Batch Size: {args.per_device_train_batch_size}\")\n",
    "# print(f\"Eval Batch Size: {args.per_device_eval_batch_size}\")\n",
    "# print(f\"Number of Epochs: {args.num_train_epochs}\")\n",
    "# print(f\"Learning Rate: {args.learning_rate}\")\n",
    "# print(f\"Project Root: {args.project_root}\")\n",
    "# print(f\"Training Dataset Path: {args.training_dataset_path}\")\n",
    "# print(f\"Model path: {args.model_path}\")\n",
    "# print(f\"tgt_lng: {args.tgt_lng}\")\n",
    "# print(f\"src_lng: {args.src_lng}\")\n",
    "# print(f\"Resume from checkpoint: {args.resume_from_checkpoint}\")\n",
    "# print(f\"Resume checkpoint path: {args.resume_checkpoint_path}\")\n",
    "# print(f\"r: {args.r}\")\n",
    "# print(f\"is_peft: {args.is_peft}\")\n",
    "# print(f\"is_unsloth: {args.is_unsloth}\")\n",
    "# print(f\"is_train_response_only: {args.is_train_response_only}\")\n",
    "\n",
    "# learning_rate = args.learning_rate # Learning rate for the optimizer\n",
    "# per_device_train_batch_size = args.per_device_train_batch_size  # Batch size for training per device\n",
    "# per_device_eval_batch_size = args.per_device_eval_batch_size  # Batch size for evaluation per device\n",
    "# num_train_epochs = args.num_train_epochs  # Number of epochs for training\n",
    "# training_dataset_path = args.training_dataset_path\n",
    "# project_root = args.project_root\n",
    "# model_path = args.model_path\n",
    "# resume_from_checkpoint = args.resume_from_checkpoint\n",
    "# resume_checkpoint_path = args.resume_checkpoint_path\n",
    "# src_lng = args.src_lng\n",
    "# tgt_lng = args.tgt_lng\n",
    "# r = args.r\n",
    "# is_peft = args.is_peft\n",
    "# is_unsloth = args.is_unsloth\n",
    "# is_train_response_only = args.is_train_response_only\n",
    "\n",
    "\n",
    "\n",
    "# Params need to be set\n",
    "learning_rate = 1e-6 # Learning rate for the optimizer\n",
    "per_device_train_batch_size = 1  # Batch size for training per device\n",
    "per_device_eval_batch_size = 1  # Batch size for evaluation per device\n",
    "num_train_epochs = 5  # Number of epochs for training\n",
    "training_dataset_path = \"data/training_dataset/dataset_GPT_split.jsonl\"\n",
    "project_root = \"/home/snt/projects_lujun/mt_luxembourgish\"\n",
    "model_path = \"/home/snt/llm_models/gemma-2-2b-it\"\n",
    "resume_from_checkpoint = False\n",
    "resume_checkpoint_path = None\n",
    "src_lng = \"English\"\n",
    "tgt_lng = \"Luxembourgish\"\n",
    "r = 32\n",
    "is_peft = False\n",
    "is_unsloth = False\n",
    "is_train_response_only = True\n",
    "\n",
    "if is_unsloth:\n",
    "    from unsloth import is_bfloat16_supported\n",
    "    from unsloth.chat_templates import train_on_responses_only\n",
    "    from unsloth import FastLanguageModel\n",
    "    fp16 = not is_bfloat16_supported()\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "else:\n",
    "    fp16 = False\n",
    "    bf16 = False\n",
    "\n",
    "\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "train_ratio = 0.001  # Number of samples to be used for training and evaluation\n",
    "warmup_ratio = 0.5\n",
    "logging_steps = 10\n",
    "evaluation_strategy=\"steps\"\n",
    "save_strategy=\"epoch\"\n",
    "eval_steps=10\n",
    "max_grad_norm = 0.3\n",
    "MAX_LEN = 512\n",
    "weight_decay = 0.01\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "train_seed = 3407\n",
    "\n",
    "if is_peft:\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\",]\n",
    "    lora_alpha = 8\n",
    "    lora_dropout = 0\n",
    "    random_state = 3407\n",
    "\n",
    "current = time.time()\n",
    "formatted_time = time.strftime(\"%m_%d_%H_%M\", time.localtime(current))\n",
    "if resume_from_checkpoint:\n",
    "    output_dir = resume_checkpoint_path\n",
    "else:\n",
    "    if is_peft:\n",
    "        input_file_name = training_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        output_dir = f\"logs/peft_{r}_{src_lng[:2]}_{tgt_lng[:2]}/fit_{formatted_time}_{train_ratio}_{input_file_name}\"\n",
    "    else:\n",
    "        input_file_name = training_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        output_dir = f\"logs/full_{src_lng[:2]}_{tgt_lng[:2]}/fit_{formatted_time}_{train_ratio}_{input_file_name}\"\n",
    "\n",
    "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
    "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n",
    "\n",
    "\n",
    "\n",
    "# ========================== dataset preparation ==========================\n",
    "\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "train_dataset_df = pd.read_json(train_dataset_path, lines=True)\n",
    "pre_processed_dataset_df = pre_process(train_dataset_df)\n",
    "\n",
    "if not isinstance(pre_processed_dataset_df, pd.DataFrame):\n",
    "    raise TypeError(\"data_preprocess should return a pandas DataFrame.\")\n",
    "\n",
    "dataset = Dataset.from_pandas(pre_processed_dataset_df)\n",
    "\n",
    "# Filter by split\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "val_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Select subset\n",
    "train_dataset = train_dataset.select(range(int(len(train_dataset) * train_ratio)))\n",
    "val_dataset = val_dataset.select(range(int(len(val_dataset) * train_ratio)))  # Avoid out-of-range error\n",
    "\n",
    "# Rename columns\n",
    "train_dataset = train_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"input\": \"Luxembourgish\",\n",
    "    \"translated_text\": \"English\",\n",
    "})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if \"gemma\" in model_path:\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda sample: {\n",
    "            \"full_prompt\": create_prompt_gemma(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "        }\n",
    "    ).select_columns([\"full_prompt\"])\n",
    "\n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda sample: {\n",
    "            \"full_prompt\": create_prompt_gemma(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "        }\n",
    "    ).select_columns([\"full_prompt\"])\n",
    "else:\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda sample: {\n",
    "            \"full_prompt\": create_prompt(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "        }\n",
    "    ).select_columns([\"full_prompt\"])\n",
    "\n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda sample: {\n",
    "            \"full_prompt\": create_prompt(sample, src_lng=src_lng, tgt_lng=tgt_lng, mode=\"train\", tokenizer=tokenizer)[\"full_prompt\"]\n",
    "        }\n",
    "    ).select_columns([\"full_prompt\"])\n",
    "\n",
    "print (train_dataset[\"full_prompt\"][0])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"full_prompt\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"full_prompt\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"full_prompt\"])\n",
    "\n",
    "\n",
    "# Using Unsloth Acceleration \n",
    "if is_unsloth:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = f\"unsloth/{model_name}\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "        # model_name = model_path,\n",
    "        max_seq_length = MAX_LEN,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    \n",
    "    if is_peft:\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r = r, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules = target_modules,\n",
    "            lora_alpha = lora_alpha,\n",
    "            lora_dropout = lora_dropout, # Supports any, but = 0 is optimized\n",
    "            bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "            random_state = random_state,\n",
    "            use_rslora = False,  # We support rank stabilized LoRA\n",
    "            loftq_config = None, # And without LoftQ\n",
    "        )\n",
    "\n",
    "# Using transformer huggingface   \n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    model.config.use_cache = False\n",
    "    if is_peft:\n",
    "        lora_config = LoraConfig(\n",
    "            r=r, \n",
    "            target_modules=target_modules, \n",
    "            lora_alpha=lora_alpha, \n",
    "            lora_dropout=lora_dropout, \n",
    "            bias=\"none\", \n",
    "            random_state=random_state,\n",
    "            use_rslora=False,\n",
    "            loftq_config=None  # And without LoftQ\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "print (model)\n",
    "print(print_trainable_parameters(model))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, Shape: {param.shape}, Trainable parameters: {param.numel()}\")\n",
    "    else:\n",
    "        print(f\"Layer: {name}, Shape: {param.shape}, Non-trainable parameters: {param.numel()}\")\n",
    "\n",
    "\n",
    "def formatting_func(example):\n",
    "  output_texts = []\n",
    "  output_texts.append(example['full_prompt'])\n",
    "  return output_texts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_ddp_accelerate_sft():\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        save_strategy=save_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16 = fp16,\n",
    "        bf16 =bf16,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"tensorboard\",\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=False,\n",
    "        seed = train_seed,\n",
    "        ddp_find_unused_parameters=False, # Avoids warnings\n",
    "        dataloader_num_workers=4,  # Adjust number of workers based on hardware\n",
    "        dataset_text_field = \"full_prompt\",\n",
    "        max_seq_length = MAX_LEN,\n",
    "        dataset_num_proc = 2,\n",
    "        packing = False, # Can make training 5x faster for short sequences.\n",
    "        # load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    if \"gemma\" in model_path:\n",
    "        print(\"Using GEMMA model.\")\n",
    "        ## without collator\n",
    "        trainer = SFTTrainer(\n",
    "            model = model,\n",
    "            train_dataset = tokenized_train_dataset,\n",
    "            eval_dataset = tokenized_val_dataset,\n",
    "            args = training_args,\n",
    "            # formatting_func = formatting_func\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using Llama model.\")\n",
    "        trainer = SFTTrainer(\n",
    "            model = model,\n",
    "            train_dataset = tokenized_train_dataset,\n",
    "            eval_dataset = tokenized_val_dataset,\n",
    "            # data_collator = data_collator,\n",
    "            args = training_args,\n",
    "        )\n",
    "\n",
    "    if is_train_response_only:\n",
    "        if \"gemma\" not in model_path and is_unsloth:\n",
    "            trainer = train_on_responses_only(\n",
    "                trainer,\n",
    "                instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "                response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            )\n",
    "            \n",
    "\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    print(\"Finished training SFT.\")\n",
    "    return trainer_stats\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "\n",
    "trainer_stats = None\n",
    "\n",
    "def main():\n",
    "    trainer_stats = train_ddp_accelerate_sft()\n",
    "    return trainer_stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer_stats = main()\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
